{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a3f2557-9ede-4e18-9a2a-4c81ff4ddd76",
   "metadata": {},
   "source": [
    "##### pip install langsmith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0352d93d-7940-4c5f-b678-e7d28910738e",
   "metadata": {},
   "source": [
    "##### pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f5bf60-5a6c-48a9-bfe9-9935bd28541e",
   "metadata": {},
   "source": [
    "#### LLM chain using local open source models using llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "575dd8ce-d251-4bce-8f1c-0c4f168aa2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are many excellent LLM (Large Language Model) models out there, each with their own strengths and weaknesses. Here are some notable ones:\\n\\n1. **BERT** (Bidirectional Encoder Representations from Transformers): Developed by Google, BERT is a pioneering model that has set the bar high for many other LLMs. It\\'s widely used in NLP tasks like question answering, sentiment analysis, and text classification.\\n2. **RoBERTa** (Robustly Optimized BERT Pre-training Approach): Another Google-developed model, RoBERTa is an improvement over BERT, with better performance on long-range dependencies and more efficient training procedures.\\n3. **Transformer-XL**: This model, developed by the Google Brain team, is a larger and more powerful version of BERT. It\\'s designed for longer-range dependencies and has achieved state-of-the-art results in various NLP tasks.\\n4. **Longformer**: Developed by Facebook AI, Longformer is a specialized LLM designed specifically for long-range dependencies and has shown excellent performance on tasks like text classification and question answering.\\n5. **T5** (Text-to-Text Transfer Transformer): This model, developed by Google, is designed to perform multi-task learning and can be fine-tuned for various NLP tasks like language translation, summarization, and text generation.\\n6. **DPR** (Dense Passage Retrieval): Developed by the Allen Institute for Artificial Intelligence, DPR is a powerful LLM specifically designed for passage-based question answering and has achieved state-of-the-art results in this task.\\n\\nSome notable features that distinguish these models include:\\n\\n* **Bigger models**: Many modern LLMs have increased their capacity to handle longer input sequences, more complex relationships, and larger training datasets.\\n* **Multi-task learning**: Some models are designed to learn multiple tasks simultaneously, which can lead to better performance and more efficient fine-tuning for specific NLP applications.\\n* **Transfer learning**: LLMs often leverage pre-trained weights and adapt them to new tasks through fine-tuning, allowing them to generalize better across different domains and datasets.\\n\\nKeep in mind that the \"best\" model depends on your specific use case, dataset, and evaluation metrics. It\\'s essential to experiment with different models and architectures to find the one that works best for you!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3\")\n",
    "llm.invoke(\"what are good llm models?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975f007b-22df-42c5-828e-d3ab2134b99d",
   "metadata": {},
   "source": [
    "###### langchain_core.prompts is a module from the LangChain library designed to facilitate the creation and manipulation of prompts for language models. ChatPromptTemplate is a specific class within this module that helps structure chat-based interactions with a language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2058f0d-2ef5-4629-bc1f-d7e5490102f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As your helpful assistant, I'd be happy to give you some insights on popular LLM (Large Language Model) models!\\n\\nThere are many excellent LLMs out there, and the best one for you depends on your specific needs and goals. Here are a few notable ones:\\n\\n1. **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google, BERT is a groundbreaking model that has achieved state-of-the-art results in various NLP tasks. It's widely used for natural language processing, question-answering, and text classification.\\n2. **RoBERTa (Robustly Optimized BERT Pretraining Approach)**: Another impressive model from the same authors as BERT, RoBERTa is designed to handle longer sequences and has been shown to perform well on various tasks like sentence classification and sentiment analysis.\\n3. **DistilBERT**: This compact version of BERT is trained using a distillation process, which enables it to be smaller (only 40% the size of the original BERT) while still retaining most of its performance capabilities.\\n4. **Longformer**: As its name suggests, Longformer is designed specifically for processing longer sequences than traditional LLMs. It's particularly useful for tasks like text classification and sentiment analysis.\\n5. **Electra (Enhind LanguagE with ConTrastive examples)**: This model uses a novel approach to improve language understanding by training it on contrastive examples. It has shown promising results in various NLP applications.\\n\\nThese are just a few examples of the many excellent LLMs out there. When choosing an LLM, consider factors like:\\n\\n* Your specific task or application\\n* The size and complexity of your dataset\\n* Computational resources (training time and memory requirements)\\n* Desired performance metrics (e.g., accuracy, speed)\\n\\nFeel free to ask me more about these models or explore other options that might suit your needs!\\n\\nWhat's your primary interest in LLMs?\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# from langchain_core.prompts import BaseMessagePromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",  \"You are a helpful assistant.\"),\n",
    "        (\"user\", \"{user_input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = template | llm \n",
    "chain.invoke({\"user_input\": \"what are good llm models?\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db97fd6-402a-4477-aa51-ef67c8898850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87bf36d1-8495-4f63-bf08-754eed63ac9a",
   "metadata": {},
   "source": [
    "##### The StrOutputParser in the langchain_core.output_parsers module is used to parse the output of a language model into a string format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78d6c2e7-659b-47ed-9c3b-2d487327c68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm glad you asked!\\n\\nThere are many excellent LLM (Large Language Model) models out there, each with their own strengths and specialties. Here are some popular ones:\\n\\n1. **BERT (Bidirectional Encoder Representations from Transformers)**: A widely used and highly effective model for natural language processing tasks like question answering, sentiment analysis, and text classification.\\n2. **RoBERTa (Robustly Optimized BERT Pretraining Approach)**: An improved version of BERT that uses a different approach to optimize the model's performance on masked language modeling.\\n3. **DistilBERT**: A smaller, more efficient version of BERT designed for deployment in production environments.\\n4. **XLNet**: A general-purpose LLM that excels at tasks like machine translation, text summarization, and question answering.\\n5. **T5 (Text-to-Text Transfer with Cross-Attention)**: A model that can be fine-tuned for various NLP tasks, including text generation, machine translation, and text classification.\\n6. **Longformer**: A model designed specifically for long-range dependencies in natural language processing, making it suitable for tasks like document-level classification and sentiment analysis.\\n7. **DeBERTa**: An LLM that uses a different approach to optimize the model's performance on masked language modeling and has achieved state-of-the-art results on several NLP benchmarks.\\n\\nThese are just a few examples of the many great LLM models out there. The choice ultimately depends on your specific use case, the task you want to perform, and the level of computational resources you have available.\\n\\nDo you have any specific requirements or goals in mind? I'd be happy to help you narrow down the options!\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# In the context of building a language model application with LangChain, the expression chain = template | llm | parser represents a pipeline where\n",
    "\n",
    "# A prompt template is used to generate a prompt.\n",
    "# The prompt is passed to a language model (LLM) to generate a response.\n",
    "# The generated response is parsed into a desired format.\n",
    "chain = template | llm | parser\n",
    "chain.invoke({\"user_input\": \"what are good llm models?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145a1a2d-a525-42ab-bf4b-ca3c19155239",
   "metadata": {},
   "source": [
    "##### pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ae03b4-f86c-4cdd-8853-ef32f8a653a6",
   "metadata": {},
   "source": [
    "###### BeautifulSoup integrates well with other Python libraries, such as requests for making HTTP requests to fetch web pages and pandas for data manipulation and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae016432-bf99-4cf9-9d0a-a9230a14b586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#bodyContent\n",
      "/wiki/Main_Page\n",
      "/wiki/Wikipedia:Contents\n",
      "/wiki/Portal:Current_events\n",
      "/wiki/Special:Random\n",
      "/wiki/Wikipedia:About\n",
      "//en.wikipedia.org/wiki/Wikipedia:Contact_us\n",
      "https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en\n",
      "/wiki/Help:Contents\n",
      "/wiki/Help:Introduction\n",
      "/wiki/Wikipedia:Community_portal\n",
      "/wiki/Special:RecentChanges\n",
      "/wiki/Wikipedia:File_upload_wizard\n",
      "/wiki/Main_Page\n",
      "/wiki/Special:Search\n",
      "/w/index.php?title=Special:CreateAccount&returnto=Large+language+model\n",
      "/w/index.php?title=Special:UserLogin&returnto=Large+language+model\n",
      "/w/index.php?title=Special:CreateAccount&returnto=Large+language+model\n",
      "/w/index.php?title=Special:UserLogin&returnto=Large+language+model\n",
      "/wiki/Help:Introduction\n",
      "/wiki/Special:MyContributions\n",
      "/wiki/Special:MyTalk\n",
      "#\n",
      "#History\n",
      "#Alternative_architecture\n",
      "#Dataset_preprocessing\n",
      "#Probabilistic_tokenization\n",
      "#BPE\n",
      "#Dataset_cleaning\n",
      "#Synthetic_data\n",
      "#Training_and_architecture\n",
      "#Reinforcement_learning_from_human_feedback_(RLHF)\n",
      "#Instruction_tuning\n",
      "#Mixture_of_experts\n",
      "#Prompt_engineering,_attention_mechanism,_and_context_window\n",
      "#Training_cost\n",
      "#Tool_use\n",
      "#Agency\n",
      "#Compression\n",
      "#Multimodality\n",
      "#Properties\n",
      "#Scaling_laws\n",
      "#Emergent_abilities\n",
      "#Interpretation\n",
      "#Understanding_and_intelligence\n",
      "#Evaluation\n",
      "#Perplexity\n",
      "#BPW,_BPC,_and_BPT\n",
      "#Task-specific_datasets_and_benchmarks\n",
      "#Adversarially_constructed_evaluations\n",
      "#Wider_impact\n",
      "#Memorization_and_copyright\n",
      "#Security\n",
      "#Algorithmic_bias\n",
      "#Stereotyping\n",
      "#Political_bias\n",
      "#List\n",
      "#See_also\n",
      "#Notes\n",
      "#References\n",
      "#Further_reading\n",
      "https://af.wikipedia.org/wiki/Groot_taalmodel\n",
      "https://ar.wikipedia.org/wiki/%D9%86%D9%85%D9%88%D8%B0%D8%AC_%D8%A7%D9%84%D9%84%D8%BA%D8%A9_%D8%A7%D9%84%D9%83%D8%A8%D9%8A%D8%B1\n",
      "https://az.wikipedia.org/wiki/B%C3%B6y%C3%BCk_dil_modeli\n",
      "https://zh-min-nan.wikipedia.org/wiki/T%C5%8Da-h%C3%AAng_g%C3%AD-gi%C3%A2n_b%C3%B4%CD%98-h%C3%AAng\n",
      "https://bar.wikipedia.org/wiki/Large_language_model\n",
      "https://bs.wikipedia.org/wiki/Veliki_jezi%C4%8Dki_modeli\n",
      "https://ca.wikipedia.org/wiki/Model_de_llenguatge_extens\n",
      "https://cs.wikipedia.org/wiki/Velk%C3%BD_jazykov%C3%BD_model\n",
      "https://de.wikipedia.org/wiki/Large_Language_Model\n",
      "https://el.wikipedia.org/wiki/%CE%9C%CE%B5%CE%B3%CE%AC%CE%BB%CE%BF_%CE%B3%CE%BB%CF%89%CF%83%CF%83%CE%B9%CE%BA%CF%8C_%CE%BC%CE%BF%CE%BD%CF%84%CE%AD%CE%BB%CE%BF\n",
      "https://es.wikipedia.org/wiki/Modelo_de_lenguaje_grande\n",
      "https://eu.wikipedia.org/wiki/Hizkuntza_Eredu_Handiak_(LLM)\n",
      "https://fa.wikipedia.org/wiki/%D9%85%D8%AF%D9%84_%D8%B2%D8%A8%D8%A7%D9%86%DB%8C_%D8%A8%D8%B2%D8%B1%DA%AF\n",
      "https://fr.wikipedia.org/wiki/Grand_mod%C3%A8le_de_langage\n",
      "https://ga.wikipedia.org/wiki/Samhail_teanga_mh%C3%B3r\n",
      "https://gl.wikipedia.org/wiki/Modelo_de_linguaxe_de_grande_escala\n",
      "https://ko.wikipedia.org/wiki/%EB%8C%80%ED%98%95_%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8\n",
      "https://hi.wikipedia.org/wiki/%E0%A4%AC%E0%A4%A1%E0%A4%BC%E0%A5%87_%E0%A4%AD%E0%A4%BE%E0%A4%B7%E0%A4%BE_%E0%A4%AE%E0%A5%89%E0%A4%A1%E0%A4%B2\n",
      "https://id.wikipedia.org/wiki/Model_bahasa_besar\n",
      "https://zu.wikipedia.org/wiki/UNongo_lolimi_olukhulu\n",
      "https://it.wikipedia.org/wiki/Modello_linguistico_di_grandi_dimensioni\n",
      "https://he.wikipedia.org/wiki/%D7%9E%D7%95%D7%93%D7%9C_%D7%A9%D7%A4%D7%94_%D7%92%D7%93%D7%95%D7%9C\n",
      "https://mk.wikipedia.org/wiki/%D0%93%D0%BE%D0%BB%D0%B5%D0%BC_%D1%98%D0%B0%D0%B7%D0%B8%D1%87%D0%B5%D0%BD_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB\n",
      "https://nl.wikipedia.org/wiki/Groot_taalmodel\n",
      "https://ja.wikipedia.org/wiki/%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB\n",
      "https://pl.wikipedia.org/wiki/Du%C5%BCy_model_j%C4%99zykowy\n",
      "https://pt.wikipedia.org/wiki/Grandes_modelos_de_linguagem\n",
      "https://qu.wikipedia.org/wiki/Hatun_simi_wallpama\n",
      "https://ru.wikipedia.org/wiki/%D0%91%D0%BE%D0%BB%D1%8C%D1%88%D0%B0%D1%8F_%D1%8F%D0%B7%D1%8B%D0%BA%D0%BE%D0%B2%D0%B0%D1%8F_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C\n",
      "https://sl.wikipedia.org/wiki/Obse%C5%BEni_jezikovni_model\n",
      "https://sr.wikipedia.org/wiki/Veliki_jezi%C4%8Dki_modeli\n",
      "https://tl.wikipedia.org/wiki/Malaking_modelong_pangwika\n",
      "https://tr.wikipedia.org/wiki/Geni%C5%9F_dil_modeli\n",
      "https://uk.wikipedia.org/wiki/%D0%92%D0%B5%D0%BB%D0%B8%D0%BA%D0%B0_%D0%BC%D0%BE%D0%B2%D0%BD%D0%B0_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C\n",
      "https://ug.wikipedia.org/wiki/%DA%86%D9%88%DA%AD_%D8%AA%D9%89%D9%84_%D9%85%D9%88%D8%AF%D9%89%D9%84%D9%89\n",
      "https://vi.wikipedia.org/wiki/M%C3%B4_h%C3%ACnh_ng%C3%B4n_ng%E1%BB%AF_l%E1%BB%9Bn\n",
      "https://zh-yue.wikipedia.org/wiki/%E5%A4%A7%E5%9E%8B%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8B\n",
      "https://zh.wikipedia.org/wiki/%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q115305900#sitelinks-wikipedia\n",
      "/wiki/Large_language_model\n",
      "/wiki/Talk:Large_language_model\n",
      "/wiki/Large_language_model\n",
      "/w/index.php?title=Large_language_model&action=edit\n",
      "/w/index.php?title=Large_language_model&action=history\n",
      "/wiki/Large_language_model\n",
      "/w/index.php?title=Large_language_model&action=edit\n",
      "/w/index.php?title=Large_language_model&action=history\n",
      "/wiki/Special:WhatLinksHere/Large_language_model\n",
      "/wiki/Special:RecentChangesLinked/Large_language_model\n",
      "/wiki/Wikipedia:File_Upload_Wizard\n",
      "/wiki/Special:SpecialPages\n",
      "/w/index.php?title=Large_language_model&oldid=1225022647\n",
      "/w/index.php?title=Large_language_model&action=info\n",
      "/w/index.php?title=Special:CiteThisPage&page=Large_language_model&id=1225022647&wpFormIdentifier=titleform\n",
      "/w/index.php?title=Special:UrlShortener&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FLarge_language_model\n",
      "/w/index.php?title=Special:QrCode&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FLarge_language_model\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q115305900\n",
      "/w/index.php?title=Special:DownloadAsPdf&page=Large_language_model&action=show-download-screen\n",
      "/w/index.php?title=Large_language_model&printable=yes\n",
      "/wiki/Machine_learning\n",
      "/wiki/Data_mining\n",
      "/wiki/Supervised_learning\n",
      "/wiki/Unsupervised_learning\n",
      "/wiki/Online_machine_learning\n",
      "/wiki/Batch_learning\n",
      "/wiki/Meta-learning_(computer_science)\n",
      "/wiki/Semi-supervised_learning\n",
      "/wiki/Self-supervised_learning\n",
      "/wiki/Reinforcement_learning\n",
      "/wiki/Curriculum_learning\n",
      "/wiki/Rule-based_machine_learning\n",
      "/wiki/Quantum_machine_learning\n",
      "/wiki/Statistical_classification\n",
      "/wiki/Generative_model\n",
      "/wiki/Regression_analysis\n",
      "/wiki/Cluster_analysis\n",
      "/wiki/Dimensionality_reduction\n",
      "/wiki/Density_estimation\n",
      "/wiki/Anomaly_detection\n",
      "/wiki/Data_cleaning\n",
      "/wiki/Automated_machine_learning\n",
      "/wiki/Association_rule_learning\n",
      "/wiki/Semantic_analysis_(machine_learning)\n",
      "/wiki/Structured_prediction\n",
      "/wiki/Feature_engineering\n",
      "/wiki/Feature_learning\n",
      "/wiki/Learning_to_rank\n",
      "/wiki/Grammar_induction\n",
      "/wiki/Ontology_learning\n",
      "/wiki/Multimodal_learning\n",
      "/wiki/Supervised_learning\n",
      "/wiki/Statistical_classification\n",
      "/wiki/Regression_analysis\n",
      "/wiki/Apprenticeship_learning\n",
      "/wiki/Decision_tree_learning\n",
      "/wiki/Ensemble_learning\n",
      "/wiki/Bootstrap_aggregating\n",
      "/wiki/Boosting_(machine_learning)\n",
      "/wiki/Random_forest\n",
      "/wiki/K-nearest_neighbors_algorithm\n",
      "/wiki/Linear_regression\n",
      "/wiki/Naive_Bayes_classifier\n",
      "/wiki/Artificial_neural_network\n",
      "/wiki/Logistic_regression\n",
      "/wiki/Perceptron\n",
      "/wiki/Relevance_vector_machine\n",
      "/wiki/Support_vector_machine\n",
      "/wiki/Cluster_analysis\n",
      "/wiki/BIRCH\n",
      "/wiki/CURE_algorithm\n",
      "/wiki/Hierarchical_clustering\n",
      "/wiki/K-means_clustering\n",
      "/wiki/Fuzzy_clustering\n",
      "/wiki/Expectation%E2%80%93maximization_algorithm\n",
      "/wiki/DBSCAN\n",
      "/wiki/OPTICS_algorithm\n",
      "/wiki/Mean_shift\n",
      "/wiki/Dimensionality_reduction\n",
      "/wiki/Factor_analysis\n",
      "/wiki/Canonical_correlation\n",
      "/wiki/Independent_component_analysis\n",
      "/wiki/Linear_discriminant_analysis\n",
      "/wiki/Non-negative_matrix_factorization\n",
      "/wiki/Principal_component_analysis\n",
      "/wiki/Proper_generalized_decomposition\n",
      "/wiki/T-distributed_stochastic_neighbor_embedding\n",
      "/wiki/Sparse_dictionary_learning\n",
      "/wiki/Structured_prediction\n",
      "/wiki/Graphical_model\n",
      "/wiki/Bayesian_network\n",
      "/wiki/Conditional_random_field\n",
      "/wiki/Hidden_Markov_model\n",
      "/wiki/Anomaly_detection\n",
      "/wiki/Random_sample_consensus\n",
      "/wiki/K-nearest_neighbors_algorithm\n",
      "/wiki/Local_outlier_factor\n",
      "/wiki/Isolation_forest\n",
      "/wiki/Artificial_neural_network\n",
      "/wiki/Autoencoder\n",
      "/wiki/Cognitive_computing\n",
      "/wiki/Deep_learning\n",
      "/wiki/DeepDream\n",
      "/wiki/Feedforward_neural_network\n",
      "/wiki/Recurrent_neural_network\n",
      "/wiki/Long_short-term_memory\n",
      "/wiki/Gated_recurrent_unit\n",
      "/wiki/Echo_state_network\n",
      "/wiki/Reservoir_computing\n",
      "/wiki/Restricted_Boltzmann_machine\n",
      "/wiki/Generative_adversarial_network\n",
      "/wiki/Diffusion_model\n",
      "/wiki/Self-organizing_map\n",
      "/wiki/Convolutional_neural_network\n",
      "/wiki/U-Net\n",
      "/wiki/Transformer_(machine_learning_model)\n",
      "/wiki/Vision_transformer\n",
      "/wiki/Mamba_(deep_learning_architecture)\n",
      "/wiki/Spiking_neural_network\n",
      "/wiki/Memtransistor\n",
      "/wiki/Electrochemical_RAM\n",
      "/wiki/Reinforcement_learning\n",
      "/wiki/Q-learning\n",
      "/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action\n",
      "/wiki/Temporal_difference_learning\n",
      "/wiki/Multi-agent_reinforcement_learning\n",
      "/wiki/Self-play_(reinforcement_learning_technique)\n",
      "/wiki/Active_learning_(machine_learning)\n",
      "/wiki/Crowdsourcing\n",
      "/wiki/Human-in-the-loop\n",
      "/wiki/Reinforcement_learning_from_human_feedback\n",
      "/wiki/Coefficient_of_determination\n",
      "/wiki/Confusion_matrix\n",
      "/wiki/Learning_curve_(machine_learning)\n",
      "/wiki/Receiver_operating_characteristic\n",
      "/wiki/Kernel_machines\n",
      "/wiki/Bias%E2%80%93variance_tradeoff\n",
      "/wiki/Computational_learning_theory\n",
      "/wiki/Empirical_risk_minimization\n",
      "/wiki/Occam_learning\n",
      "/wiki/Probably_approximately_correct_learning\n",
      "/wiki/Statistical_learning_theory\n",
      "/wiki/Vapnik%E2%80%93Chervonenkis_theory\n",
      "/wiki/ECML_PKDD\n",
      "/wiki/Conference_on_Neural_Information_Processing_Systems\n",
      "/wiki/International_Conference_on_Machine_Learning\n",
      "/wiki/International_Conference_on_Learning_Representations\n",
      "/wiki/International_Joint_Conference_on_Artificial_Intelligence\n",
      "/wiki/Machine_Learning_(journal)\n",
      "/wiki/Journal_of_Machine_Learning_Research\n",
      "/wiki/Glossary_of_artificial_intelligence\n",
      "/wiki/List_of_datasets_for_machine-learning_research\n",
      "/wiki/List_of_datasets_in_computer_vision_and_image_processing\n",
      "/wiki/Outline_of_machine_learning\n",
      "/wiki/Template:Machine_learning\n",
      "/wiki/Template_talk:Machine_learning\n",
      "/wiki/Special:EditPage/Template:Machine_learning\n",
      "/wiki/Language_model\n",
      "/wiki/Self-supervised_learning\n",
      "/wiki/Semi-supervised_learning\n",
      "#cite_note-:7-1\n",
      "/wiki/Generative_artificial_intelligence\n",
      "#cite_note-Bowman-2\n",
      "/wiki/Artificial_neural_network\n",
      "https://en.wikipedia.org/w/index.php?title=Large_language_model&action=edit\n",
      "/wiki/Transformer_(deep_learning_architecture)\n",
      "/wiki/Fine-tuning_(deep_learning)\n",
      "/wiki/GPT-3\n",
      "/wiki/Prompt_engineering\n",
      "#cite_note-few-shot-learners-3\n",
      "/wiki/Algorithmic_bias\n",
      "#cite_note-Manning-2022-4\n",
      "/wiki/OpenAI\n",
      "/wiki/Generative_pre-trained_transformer\n",
      "/wiki/GPT-3.5\n",
      "/wiki/GPT-4\n",
      "/wiki/ChatGPT\n",
      "/wiki/Microsoft_Copilot\n",
      "/wiki/Google\n",
      "/wiki/Gemini_(language_model)\n",
      "/wiki/Gemini_(chatbot)\n",
      "/wiki/Meta_Platforms\n",
      "/wiki/LLaMA\n",
      "/wiki/Anthropic\n",
      "/wiki/Claude_(language_model)\n",
      "/wiki/Mistral_AI\n",
      "/w/index.php?title=Large_language_model&action=edit&section=1\n",
      "/wiki/File:The-Transformer-model-architecture.png\n",
      "/wiki/NeurIPS\n",
      "/wiki/Transformer_architecture\n",
      "/wiki/Attention_Is_All_You_Need\n",
      "/wiki/Seq2seq\n",
      "#cite_note-5\n",
      "/wiki/Attention_(machine_learning)\n",
      "#cite_note-6\n",
      "/wiki/BERT_(language_model)\n",
      "#cite_note-7\n",
      "/wiki/GPT-1\n",
      "/wiki/GPT-2\n",
      "/wiki/OpenAI\n",
      "#cite_note-8\n",
      "/wiki/GPT-3\n",
      "https://en.wikipedia.org/w/index.php?title=Large_language_model&action=edit\n",
      "/wiki/Web_API\n",
      "/wiki/ChatGPT\n",
      "#cite_note-9\n",
      "/wiki/GPT-4\n",
      "/wiki/Multimodal_learning\n",
      "#cite_note-10\n",
      "/wiki/Parameter#Artificial_Intelligence\n",
      "#cite_note-11\n",
      "/wiki/Source-available_software\n",
      "/wiki/BLOOM_(language_model)\n",
      "/wiki/LLaMA\n",
      "/wiki/Mistral_AI\n",
      "/wiki/Apache_License\n",
      "https://en.wikipedia.org/w/index.php?title=Large_language_model&action=edit\n",
      "#cite_note-12\n",
      "/w/index.php?title=Large_language_model&action=edit&section=2\n",
      "/wiki/Recurrent_neural_network\n",
      "/wiki/Mamba_(deep_learning_architecture)\n",
      "/wiki/State-space_representation\n",
      "#cite_note-13\n",
      "#cite_note-14\n",
      "#cite_note-15\n",
      "/w/index.php?title=Large_language_model&action=edit&section=3\n",
      "/wiki/List_of_datasets_for_machine-learning_research#Internet\n",
      "/w/index.php?title=Large_language_model&action=edit&section=4\n",
      "/wiki/Machine_learning\n",
      "/wiki/Word_embedding\n",
      "/wiki/Byte_pair_encoding\n",
      "/wiki/BERT_(language_model)#Design\n",
      "/wiki/Data_compression\n",
      "/wiki/Array_(data_structure)\n",
      "/wiki/Jagged_array\n",
      "#cite_note-16\n",
      "#cite_note-17\n",
      "/w/index.php?title=Large_language_model&action=edit&section=5\n",
      "/wiki/Punctuation_mark\n",
      "/wiki/N-gram\n",
      "/wiki/GPT-3\n",
      "#cite_note-xbiWb-18\n",
      "/wiki/Integers\n",
      "#cite_note-2022Book_-19\n",
      "/wiki/Shan_language\n",
      "/wiki/Myanmar\n",
      "#cite_note-20\n",
      "/w/index.php?title=Large_language_model&action=edit&section=6\n",
      "/wiki/Data_cleansing\n",
      "#cite_note-aYNg4-21\n",
      "#cite_note-22\n",
      "#cite_note-23\n",
      "#cite_note-24\n",
      "#cite_note-qbFw1-25\n",
      "/w/index.php?title=Large_language_model&action=edit&section=7\n",
      "/wiki/Synthetic_data\n",
      "#cite_note-26\n",
      "/w/index.php?title=Large_language_model&action=edit&section=8\n",
      "/wiki/Fine-tuning_(machine_learning)\n",
      "/w/index.php?title=Large_language_model&action=edit&section=9\n",
      "/wiki/Reinforcement_learning_from_human_feedback\n",
      "/wiki/Proximal_Policy_Optimization\n",
      "#cite_note-instructGPT-paper-27\n",
      "/w/index.php?title=Large_language_model&action=edit&section=10\n",
      "/wiki/Bootstrapping\n",
      "#cite_note-self-instruct-paper-28\n",
      "/w/index.php?title=Large_language_model&action=edit&section=11\n",
      "/wiki/Mixture_of_experts\n",
      "/wiki/Mixture_of_experts\n",
      "#cite_note-HGZCJ-29\n",
      "#cite_note-R9Qq5-30\n",
      "#cite_note-glam-blog-31\n",
      "/w/index.php?title=Large_language_model&action=edit&section=12\n",
      "/wiki/Prompt_engineering\n",
      "/wiki/Attention_(machine_learning)\n",
      "/wiki/Prompt_engineering\n",
      "#cite_note-emergentpaper-32\n",
      "/wiki/File:Multiple_attention_heads.png\n",
      "#cite_note-Jay_Allamar-33\n",
      "/wiki/GPT-2\n",
      "#cite_note-Jay_Allamar_GPT2-34\n",
      "#cite_note-2022Book_-19\n",
      "/wiki/Gemini_(language_model)\n",
      "#cite_note-35\n",
      "#cite_note-36\n",
      "#cite_note-37\n",
      "/wiki/Chat-GPT\n",
      "#cite_note-ioUpE-38\n",
      "/wiki/Generative_pretrained_transformer\n",
      "/wiki/Cloze_test\n",
      "#cite_note-jm-39\n",
      "#cite_note-jm-39\n",
      "/wiki/Regularization_(mathematics)\n",
      "/wiki/Training,_validation,_and_test_data_sets\n",
      "/w/index.php?title=Large_language_model&action=edit&section=13\n",
      "/wiki/Ampere_(microarchitecture)\n",
      "#cite_note-Wiggers-40\n",
      "#cite_note-xaytj-41\n",
      "#cite_note-Pythia-42\n",
      "#cite_note-43\n",
      "/wiki/FLOPS\n",
      "#cite_note-kaplan-scaling-44\n",
      "/w/index.php?title=Large_language_model&action=edit&section=14\n",
      "#cite_note-PI1fW-45\n",
      "#cite_note-J5OW5-46\n",
      "#cite_note-gQxzq-47\n",
      "/wiki/API\n",
      "#cite_note-lLrda-48\n",
      "#cite_note-4Xzrs-49\n",
      "/wiki/Document_retrieval\n",
      "/wiki/Vector_database\n",
      "#cite_note-BUZBP-50\n",
      "/w/index.php?title=Large_language_model&action=edit&section=15\n",
      "/wiki/Intelligent_agent\n",
      "#cite_note-CFuti-51\n",
      "/wiki/Wikipedia:Citation_needed\n",
      "/wiki/Intelligent_agent\n",
      "#cite_note-DmvNE-52\n",
      "#cite_note-JS8Vd-53\n",
      "#cite_note-54\n",
      "#cite_note-sbB2T-55\n",
      "/wiki/Wikipedia:Citation_needed\n",
      "/wiki/Monte_Carlo_tree_search\n",
      "#cite_note-ltTer-56\n",
      "#cite_note-mBvD9-57\n",
      "/wiki/Zone_of_proximal_development\n",
      "/wiki/Curriculum_learning\n",
      "#cite_note-:0-58\n",
      "/wiki/Function_(computer_programming)\n",
      "#cite_note-:0-58\n",
      "#cite_note-XuvjF-59\n",
      "/w/index.php?title=Large_language_model&action=edit&section=16\n",
      "#cite_note-60\n",
      "/wiki/Quantization_(signal_processing)\n",
      "#cite_note-LS2Go-61\n",
      "#cite_note-cpzcK-62\n",
      "#cite_note-QVU95-63\n",
      "/wiki/Block_cipher\n",
      "/wiki/Mixed-precision_arithmetic\n",
      "#cite_note-dU9Bu-64\n",
      "#cite_note-D0nFA-65\n",
      "/w/index.php?title=Large_language_model&action=edit&section=17\n",
      "/wiki/Modality_(human%E2%80%93computer_interaction)\n",
      "/wiki/Proprioception\n",
      "#cite_note-66\n",
      "/wiki/AlexNet\n",
      "#cite_note-67\n",
      "/wiki/Visual_question_answering\n",
      "#cite_note-68\n",
      "/wiki/Speech_recognition\n",
      "#cite_note-69\n",
      "#cite_note-70\n",
      "/wiki/Pathways_Language_Model\n",
      "#cite_note-71\n",
      "/wiki/LLaMA\n",
      "#cite_note-72\n",
      "#cite_note-73\n",
      "/wiki/GPT-4\n",
      "#cite_note-74\n",
      "#cite_note-75\n",
      "/wiki/Google_DeepMind\n",
      "/wiki/Gemini_(language_model)\n",
      "#cite_note-76\n",
      "/w/index.php?title=Large_language_model&action=edit&section=18\n",
      "/w/index.php?title=Large_language_model&action=edit&section=19\n",
      "/wiki/Neural_scaling_law\n",
      "/wiki/Artificial_neural_network\n",
      "/wiki/Empirical_statistical_laws\n",
      "/wiki/Chinchilla_AI\n",
      "/wiki/Log-log_plot\n",
      "/wiki/Learning_rate\n",
      "#cite_note-fJta3-77\n",
      "/wiki/FLOPS\n",
      "/wiki/Nat_(unit)\n",
      "#cite_note-kaplan-scaling-44\n",
      "/w/index.php?title=Large_language_model&action=edit&section=20\n",
      "/wiki/File:LLM_emergent_benchmarks.png\n",
      "/wiki/Neural_scaling_law#Broken_Neural_Scaling_Laws_(BNSL)\n",
      "#cite_note-IYm4Q-78\n",
      "/wiki/Neural_scaling_law#Broken_Neural_Scaling_Laws_(BNSL)\n",
      "#cite_note-IYm4Q-78\n",
      "#cite_note-emergentpaper-32\n",
      "#cite_note-JM6s1-79\n",
      "#cite_note-Bowman-2\n",
      "/wiki/In-context_learning\n",
      "#cite_note-Hahn_20230314-80\n",
      "/wiki/International_Phonetic_Alphabet\n",
      "#cite_note-emergentpaper-32\n",
      "#cite_note-57FEA-81\n",
      "#cite_note-TEIkA-82\n",
      "/wiki/Cardinal_direction\n",
      "#cite_note-zgy1i-83\n",
      "/wiki/Chain-of-thought_prompting\n",
      "#cite_note-Imb98-84\n",
      "/wiki/Hinglish\n",
      "/wiki/Kiswahili\n",
      "#cite_note-CeQVF-85\n",
      "/wiki/Neural_scaling_law\n",
      "#cite_note-C775b-86\n",
      "/w/index.php?title=Large_language_model&action=edit&section=21\n",
      "/wiki/Black_box\n",
      "/wiki/Reverse_engineering\n",
      "/wiki/Reversi\n",
      "#cite_note-IZSIr-87\n",
      "#cite_note-RLik9-88\n",
      "/wiki/Karel_(programming_language)\n",
      "#cite_note-Hln1l-89\n",
      "/wiki/Modular_arithmetic\n",
      "/wiki/Discrete_Fourier_transform\n",
      "#cite_note-oYGlo-90\n",
      "/w/index.php?title=Large_language_model&action=edit&section=22\n",
      "#cite_note-debate_understanding-91\n",
      "/wiki/Natural_language_understanding\n",
      "/wiki/Artificial_general_intelligence\n",
      "#cite_note-O8Upd-92\n",
      "#cite_note-microsoft_sparks-93\n",
      "#cite_note-rEEmH-94\n",
      "#cite_note-new_yorker_kind_of_mind-95\n",
      "/wiki/Shoggoth\n",
      "#cite_note-rAFIZ-96\n",
      "#cite_note-4luKE-97\n",
      "#cite_note-new_yorker_kind_of_mind-95\n",
      "/wiki/Stochastic_parrot\n",
      "#cite_note-debate_understanding-91\n",
      "#cite_note-microsoft_sparks-93\n",
      "/wiki/Justification_(epistemology)\n",
      "/wiki/Training_data\n",
      "/wiki/Hallucination_(artificial_intelligence)\n",
      "#cite_note-hallucination-survey-98\n",
      "#cite_note-99\n",
      "/wiki/Terrence_Sejnowski\n",
      "#cite_note-debate_understanding-91\n",
      "#cite_note-debate_understanding-91\n",
      "/wiki/Cognition\n",
      "/wiki/Cognitive_linguistics\n",
      "/wiki/George_Lakoff\n",
      "#cite_note-100\n",
      "/wiki/Cognitive_linguistics#Computational_approaches\n",
      "https://www.icsi.berkeley.edu/icsi/projects/ai/ntl\n",
      "/wiki/The_Language_Myth\n",
      "/wiki/Vyvyan_Evans\n",
      "/wiki/Probabilistic_context-free_grammar\n",
      "/wiki/Natural_language_processing#Cognition\n",
      "#cite_note-101\n",
      "#cite_note-102\n",
      "/w/index.php?title=Large_language_model&action=edit&section=23\n",
      "/w/index.php?title=Large_language_model&action=edit&section=24\n",
      "/wiki/Perplexity\n",
      "/wiki/Overfit\n",
      "/wiki/Test_set\n",
      "#cite_note-jm-39\n",
      "#cite_note-few-shot-learners-3\n",
      "/w/index.php?title=Large_language_model&action=edit&section=25\n",
      "/wiki/Information_theory\n",
      "/wiki/Entropy_(information_theory)\n",
      "/wiki/Claude_Shannon\n",
      "#cite_note-Huyen-103\n",
      "/wiki/Cross-entropy\n",
      "/w/index.php?title=Large_language_model&action=edit&section=26\n",
      "#cite_note-boolq-104\n",
      "#cite_note-boolq-104\n",
      "#cite_note-survey-105\n",
      "#cite_note-survey-105\n",
      "#cite_note-few-shot-learners-3\n",
      "/wiki/MMLU\n",
      "#cite_note-Huyen-103\n",
      "#cite_note-survey-105\n",
      "/w/index.php?title=Large_language_model&action=edit&section=27\n",
      "#cite_note-bigbench-106\n",
      "#cite_note-debate_understanding-91\n",
      "https://en.wiktionary.org/wiki/you_can%27t_teach_an_old_dog_new_tricks\n",
      "#cite_note-truthfulqa-107\n",
      "#cite_note-hellaswag-108\n",
      "/wiki/BERT_(language_model)\n",
      "#cite_note-hellaswag-108\n",
      "/w/index.php?title=Large_language_model&action=edit&section=28\n",
      "/wiki/Nature_Biomedical_Engineering\n",
      "#cite_note-ZDTUM-109\n",
      "/wiki/Goldman_Sachs\n",
      "#cite_note-81w7x-110\n",
      "#cite_note-zIM6Y-111\n",
      "/w/index.php?title=Large_language_model&action=edit&section=29\n",
      "/wiki/Artificial_intelligence_and_copyright\n",
      "#cite_note-112\n",
      "#cite_note-113\n",
      "/w/index.php?title=Large_language_model&action=edit&section=30\n",
      "#cite_note-nD6kH-114\n",
      "#cite_note-PKiPY-115\n",
      "/wiki/Cornell_University\n",
      "/wiki/University_of_California,_Berkeley\n",
      "/wiki/ChatGPT\n",
      "#cite_note-116\n",
      "#cite_note-117\n",
      "/w/index.php?title=Large_language_model&action=edit&section=31\n",
      "/wiki/Algorithmic_bias\n",
      "#cite_note-:8-118\n",
      "#cite_note-:1-119\n",
      "/w/index.php?title=Large_language_model&action=edit&section=32\n",
      "#cite_note-120\n",
      "#cite_note-:8-118\n",
      "#cite_note-121\n",
      "/w/index.php?title=Large_language_model&action=edit&section=33\n",
      "#cite_note-122\n",
      "/w/index.php?title=Large_language_model&action=edit&section=34\n",
      "/wiki/List_of_chatbots\n",
      "#cite_note-123\n",
      "#cite_note-124\n",
      "#cite_note-125\n",
      "/wiki/GPT-1\n",
      "/wiki/OpenAI\n",
      "#cite_note-oai-unsup-126\n",
      "#cite_note-gpt1-127\n",
      "/wiki/Graphics_processing_unit\n",
      "/wiki/BERT_(language_model)\n",
      "/wiki/Google\n",
      "#cite_note-bert-paper-128\n",
      "#cite_note-bert-paper-128\n",
      "#cite_note-bHZJ2-129\n",
      "#cite_note-bert-web-130\n",
      "#cite_note-Manning-2022-4\n",
      "#cite_note-Ir545-131\n",
      "/wiki/T5_(language_model)\n",
      "#cite_note-:6-132\n",
      "#cite_note-:6-132\n",
      "#cite_note-133\n",
      "#cite_note-134\n",
      "/wiki/Google\n",
      "#cite_note-45rAm-135\n",
      "#cite_note-xlnet-136\n",
      "#cite_note-gAbNO-137\n",
      "#cite_note-LX3rI-138\n",
      "/wiki/GPT-2\n",
      "/wiki/OpenAI\n",
      "#cite_note-15Brelease-139\n",
      "#cite_note-5T8u5-140\n",
      "#cite_note-LambdaLabs-141\n",
      "#cite_note-Sudbe-142\n",
      "/wiki/GPT-3\n",
      "#cite_note-Wiggers-40\n",
      "#cite_note-LambdaLabs-141\n",
      "#cite_note-:2-143\n",
      "/wiki/ChatGPT\n",
      "#cite_note-chatgpt-blog-144\n",
      "/wiki/EleutherAI\n",
      "#cite_note-gpt-neo-145\n",
      "#cite_note-Pile-146\n",
      "#cite_note-vb-gpt-neo-147\n",
      "/wiki/EleutherAI#GPT_models\n",
      "#cite_note-vb-gpt-neo-147\n",
      "/wiki/GPT-J\n",
      "/wiki/EleutherAI\n",
      "#cite_note-JxohJ-148\n",
      "#cite_note-Pile-146\n",
      "#cite_note-:3-149\n",
      "#cite_note-BwnW5-150\n",
      "/wiki/Microsoft\n",
      "/wiki/Nvidia\n",
      "#cite_note-mtnlg-preprint-151\n",
      "#cite_note-mtnlg-preprint-151\n",
      "/wiki/Baidu\n",
      "#cite_note-qeOB8-152\n",
      "/wiki/Ernie_Bot\n",
      "/wiki/Claude_(language_model)\n",
      "#cite_note-i8jc4-153\n",
      "/wiki/Anthropic\n",
      "#cite_note-AnthroArch-154\n",
      "#cite_note-AnthroArch-154\n",
      "#cite_note-RZqhw-155\n",
      "#cite_note-glam-blog-31\n",
      "#cite_note-glam-blog-31\n",
      "#cite_note-glam-blog-31\n",
      "/wiki/Mixture_of_experts\n",
      "/wiki/DeepMind\n",
      "#cite_note-mD5eE-156\n",
      "#cite_note-hoffman-157\n",
      "#cite_note-:4-158\n",
      "/wiki/LaMDA\n",
      "#cite_note-lamda-blog-159\n",
      "#cite_note-lamda-blog-159\n",
      "#cite_note-hoffman-157\n",
      "#cite_note-DMs9Z-160\n",
      "/wiki/EleutherAI\n",
      "#cite_note-gpt-neox-20b-161\n",
      "#cite_note-Pile-146\n",
      "#cite_note-:3-149\n",
      "/wiki/Chinchilla_AI\n",
      "/wiki/DeepMind\n",
      "#cite_note-chinchilla-blog-162\n",
      "#cite_note-chinchilla-blog-162\n",
      "#cite_note-hoffman-157\n",
      "#cite_note-:4-158\n",
      "/wiki/Sparrow_(bot)\n",
      "/wiki/Neural_scaling_law\n",
      "/wiki/PaLM\n",
      "#cite_note-palm-blog-163\n",
      "#cite_note-chinchilla-blog-162\n",
      "#cite_note-:4-158\n",
      "/wiki/Tensor_Processing_Unit\n",
      "#cite_note-:4-158\n",
      "/wiki/Meta_Platforms\n",
      "#cite_note-jlof8-164\n",
      "#cite_note-QjTIc-165\n",
      "#cite_note-:3-149\n",
      "#cite_note-166\n",
      "/wiki/Yandex\n",
      "#cite_note-yalm-repo-167\n",
      "#cite_note-yalm-repo-167\n",
      "#cite_note-minerva-paper-168\n",
      "#cite_note-minerva-paper-168\n",
      "#cite_note-FfCNK-169\n",
      "/wiki/BLOOM_(language_model)\n",
      "/wiki/Hugging_Face\n",
      "#cite_note-bigger-better-170\n",
      "#cite_note-B8wB2-171\n",
      "/wiki/Meta_Platforms\n",
      "#cite_note-37sY6-172\n",
      "/wiki/Amazon_(company)\n",
      "#cite_note-u5szh-173\n",
      "#cite_note-HaA7l-174\n",
      "#cite_note-rpehM-175\n",
      "/wiki/Neuro-sama\n",
      "/wiki/Twitch_(service)\n",
      "/wiki/LLaMA\n",
      "/wiki/Meta_AI\n",
      "#cite_note-llama-blog-176\n",
      "#cite_note-llama-blog-176\n",
      "#cite_note-:5-177\n",
      "#cite_note-178\n",
      "#cite_note-llama-blog-176\n",
      "#cite_note-KBedq-179\n",
      "/wiki/GPT-4\n",
      "#cite_note-181\n",
      "/wiki/GPT-4#Usage\n",
      "/wiki/Cerebras\n",
      "#cite_note-D0k2a-182\n",
      "#cite_note-:3-149\n",
      "/wiki/Technology_Innovation_Institute\n",
      "#cite_note-falcon-183\n",
      "#cite_note-Xb1gq-184\n",
      "#cite_note-gzTNw-185\n",
      "#cite_note-:5-177\n",
      "#cite_note-Wmlcs-186\n",
      "/wiki/Bloomberg_L.P.\n",
      "#cite_note-nGOSu-187\n",
      "/wiki/Huawei_PanGu\n",
      "/wiki/Huawei\n",
      "#cite_note-9WSFw-188\n",
      "#cite_note-JiOl8-189\n",
      "/wiki/LAION\n",
      "#cite_note-190\n",
      "/wiki/AI21_Labs\n",
      "#cite_note-191\n",
      "/wiki/PaLM\n",
      "#cite_note-cnbc-20230516-192\n",
      "#cite_note-cnbc-20230516-192\n",
      "#cite_note-:5-177\n",
      "/wiki/Bard_(chatbot)\n",
      "#cite_note-pWyLA-193\n",
      "#cite_note-meta-20230719-194\n",
      "#cite_note-meta-20230719-194\n",
      "/wiki/Claude_(language_model)\n",
      "#cite_note-195\n",
      "#cite_note-tii-20230921-196\n",
      "#cite_note-tii-20230921-196\n",
      "/wiki/Mistral_AI\n",
      "#cite_note-mistral-20230927-197\n",
      "/wiki/Claude_(language_model)\n",
      "#cite_note-198\n",
      "#cite_note-199\n",
      "/wiki/X.AI\n",
      "/wiki/Grok_(chatbot)\n",
      "#cite_note-200\n",
      "/wiki/Gemini_(language_model)\n",
      "/wiki/Google_DeepMind\n",
      "/wiki/Gemini_(chatbot)\n",
      "#cite_note-201\n",
      "/wiki/Mistral_AI\n",
      "#cite_note-202\n",
      "/wiki/Mixture_of_experts\n",
      "#cite_note-203\n",
      "/wiki/Mistral_AI\n",
      "#cite_note-204\n",
      "#cite_note-:9-205\n",
      "#cite_note-:9-205\n",
      "#cite_note-206\n",
      "/wiki/Gemini_(language_model)\n",
      "/wiki/Google_DeepMind\n",
      "#cite_note-207\n",
      "/wiki/Google_DeepMind\n",
      "#cite_note-gemma-208\n",
      "/wiki/Claude_(language_model)\n",
      "#cite_note-209\n",
      "/wiki/DBRX\n",
      "/wiki/Databricks\n",
      "/wiki/Mosaic_ML\n",
      "/wiki/Fujitsu\n",
      "/wiki/Tokyo_Institute_of_Technology\n",
      "/wiki/Fugaku_(supercomputer)\n",
      "#cite_note-210\n",
      "#cite_note-211\n",
      "/w/index.php?title=Large_language_model&action=edit&section=35\n",
      "/wiki/Foundation_models\n",
      "/w/index.php?title=Large_language_model&action=edit&section=36\n",
      "#cite_ref-123\n",
      "#cite_ref-124\n",
      "#cite_ref-125\n",
      "#cite_ref-166\n",
      "#cite_ref-178\n",
      "#cite_ref-181\n",
      "#cite_note-GPT4Tech-180\n",
      "/w/index.php?title=Large_language_model&action=edit&section=37\n",
      "#cite_ref-:7_1-0\n",
      "https://openai.com/blog/better-language-models/\n",
      "https://web.archive.org/web/20201219132206/https://openai.com/blog/better-language-models/\n",
      "#cite_ref-Bowman_2-0\n",
      "#cite_ref-Bowman_2-1\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2304.00612\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-few-shot-learners_3-0\n",
      "#cite_ref-few-shot-learners_3-1\n",
      "#cite_ref-few-shot-learners_3-2\n",
      "https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf\n",
      "#cite_ref-Manning-2022_4-0\n",
      "#cite_ref-Manning-2022_4-1\n",
      "/wiki/Christopher_D._Manning\n",
      "https://www.amacad.org/publication/human-language-understanding-reasoning\n",
      "/wiki/Doi_(identifier)\n",
      "https://doi.org/10.1162%2Fdaed_a_01905\n",
      "/wiki/S2CID_(identifier)\n",
      "https://api.semanticscholar.org/CorpusID:248377870\n",
      "#cite_ref-5\n",
      "/wiki/Ashish_Vaswani\n",
      "/wiki/Aidan_Gomez\n",
      "https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
      "#cite_ref-6\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/1409.0473\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-7\n",
      "https://aclanthology.org/2020.tacl-1.54\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2002.12327\n",
      "/wiki/Doi_(identifier)\n",
      "https://doi.org/10.1162%2Ftacl_a_00349\n",
      "/wiki/S2CID_(identifier)\n",
      "https://api.semanticscholar.org/CorpusID:211532403\n",
      "#cite_ref-8\n",
      "https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction\n",
      "/wiki/The_Guardian\n",
      "#cite_ref-9\n",
      "https://www.euronews.com/next/2023/11/30/chatgpt-a-year-on-3-ways-the-ai-chatbot-has-completely-changed-the-world-in-12-months\n",
      "/wiki/Euronews\n",
      "#cite_ref-10\n",
      "https://www.technologyreview.com/2023/03/14/1069823/gpt-4-is-bigger-and-better-chatgpt-openai/\n",
      "/wiki/MIT_Technology_Review\n",
      "#cite_ref-11\n",
      "https://ourworldindata.org/grapher/artificial-intelligence-parameter-count?time=2017-09-05..latest\n",
      "#cite_ref-12\n",
      "https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard\n",
      "#cite_ref-13\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2305.13048\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-14\n",
      "https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transformer-model/\n",
      "#cite_ref-15\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2312.00752\n",
      "#cite_ref-16\n",
      "https://blog.yenniejun.com/p/all-languages-are-not-created-tokenized\n",
      "#cite_ref-17\n",
      "https://openreview.net/forum?id=Pj4YYuxTq9\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2305.15425\n",
      "#cite_ref-xbiWb_18-0\n",
      "https://web.archive.org/web/20230423211308/https://platform.openai.com/tokenizer\n",
      "https://platform.openai.com/\n",
      "#cite_ref-2022Book_19-0\n",
      "#cite_ref-2022Book_19-1\n",
      "https://link.springer.com/chapter/10.1007/978-3-031-23190-2_2\n",
      "/wiki/Doi_(identifier)\n",
      "https://doi.org/10.1007%2F978-3-031-23190-2_2\n",
      "/wiki/ISBN_(identifier)\n",
      "/wiki/Special:BookSources/9783031231902\n",
      "#cite_ref-20\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2305.15425\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-aYNg4_21-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2104.08758\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-22\n",
      "https://aclanthology.org/2022.acl-long.577.pdf\n",
      "/wiki/Doi_(identifier)\n",
      "https://doi.org/10.18653%2Fv1%2F2022.acl-long.577\n",
      "#cite_ref-23\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2309.05463\n",
      "#cite_ref-24\n",
      "http://arxiv.org/abs/2404.07965\n",
      "/wiki/Doi_(identifier)\n",
      "https://doi.org/10.48550%2FarXiv.2404.07965\n",
      "#cite_ref-qbFw1_25-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2005.14165\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-26\n",
      "http://arxiv.org/abs/2404.14219\n",
      "/wiki/Doi_(identifier)\n",
      "https://doi.org/10.48550%2FarXiv.2404.14219\n",
      "#cite_ref-instructGPT-paper_27-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2203.02155\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-self-instruct-paper_28-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2212.10560\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-HGZCJ_29-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/1701.06538\n",
      "https://arxiv.org/archive/cs.LG\n",
      "#cite_ref-R9Qq5_30-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2006.16668\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-glam-blog_31-0\n",
      "#cite_ref-glam-blog_31-1\n",
      "#cite_ref-glam-blog_31-2\n",
      "#cite_ref-glam-blog_31-3\n",
      "https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html\n",
      "#cite_ref-emergentpaper_32-0\n",
      "#cite_ref-emergentpaper_32-1\n",
      "#cite_ref-emergentpaper_32-2\n",
      "https://openreview.net/forum?id=yzkSU5zdwD\n",
      "/wiki/ISSN_(identifier)\n",
      "https://www.worldcat.org/issn/2835-8856\n",
      "#cite_ref-Jay_Allamar_33-0\n",
      "https://jalammar.github.io/illustrated-transformer/\n",
      "#cite_ref-Jay_Allamar_GPT2_34-0\n",
      "https://jalammar.github.io/illustrated-gpt2/\n",
      "#cite_ref-35\n",
      "https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#context-window\n",
      "#cite_ref-36\n",
      "https://www.anthropic.com/news/claude-2-1-prompting\n",
      "#cite_ref-37\n",
      "https://platform.openai.com/docs/guides/rate-limits\n",
      "#cite_ref-ioUpE_38-0\n",
      "https://www.researchgate.net/publication/338931711\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2104.10810\n",
      "/wiki/Doi_(identifier)\n",
      "https://doi.org/10.1145%2F3373017.3373028\n",
      "/wiki/ISBN_(identifier)\n",
      "/wiki/Special:BookSources/9781450376976\n",
      "/wiki/S2CID_(identifier)\n",
      "https://api.semanticscholar.org/CorpusID:211040895\n",
      "#cite_ref-jm_39-0\n",
      "#cite_ref-jm_39-1\n",
      "#cite_ref-jm_39-2\n",
      "https://web.stanford.edu/~jurafsky/slp3/ed3book_jan72023.pdf\n",
      "#cite_ref-Wiggers_40-0\n",
      "#cite_ref-Wiggers_40-1\n",
      "https://techcrunch.com/2022/04/28/the-emerging-types-of-language-models-and-why-they-matter/\n",
      "#cite_ref-xaytj_41-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2004.08900\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-Pythia_42-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2304.01373\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-43\n",
      "http://arxiv.org/abs/2310.03715\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2310.03715\n",
      "#cite_ref-kaplan-scaling_44-0\n",
      "#cite_ref-kaplan-scaling_44-1\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2001.08361\n",
      "https://arxiv.org/archive/cs.LG\n",
      "#cite_ref-PI1fW_45-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2211.10435\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-J5OW5_46-0\n",
      "https://reasonwithpal.com/\n",
      "#cite_ref-gQxzq_47-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2303.09014\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-lLrda_48-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2303.16434\n",
      "https://arxiv.org/archive/cs.AI\n",
      "#cite_ref-4Xzrs_49-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2305.15334\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-BUZBP_50-0\n",
      "https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2005.11401\n",
      "#cite_ref-CFuti_51-0\n",
      "https://proceedings.mlr.press/v162/huang22a.html\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2201.07207\n",
      "#cite_ref-DmvNE_52-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2210.03629\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-JS8Vd_53-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2305.15486\n",
      "https://arxiv.org/archive/cs.AI\n",
      "#cite_ref-54\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2302.01560\n",
      "https://arxiv.org/archive/cs.AI\n",
      "#cite_ref-sbB2T_55-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2303.11366\n",
      "https://arxiv.org/archive/cs.AI\n",
      "#cite_ref-ltTer_56-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2305.14992\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-mBvD9_57-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2306.01711\n",
      "https://arxiv.org/archive/cs.AI\n",
      "#cite_ref-:0_58-0\n",
      "#cite_ref-:0_58-1\n",
      "https://voyager.minedojo.org/\n",
      "#cite_ref-XuvjF_59-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2304.03442\n",
      "https://arxiv.org/archive/cs.HC\n",
      "#cite_ref-60\n",
      "https://www.theregister.com/2024/03/17/ai_pc_local_llm/\n",
      "#cite_ref-LS2Go_61-0\n",
      "https://proceedings.mlr.press/v119/nagel20a.html\n",
      "#cite_ref-cpzcK_62-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/1802.05668\n",
      "https://arxiv.org/archive/cs.NE\n",
      "#cite_ref-QVU95_63-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2210.17323\n",
      "https://arxiv.org/archive/cs.LG\n",
      "#cite_ref-dU9Bu_64-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2306.03078\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-D0nFA_65-0\n",
      "/wiki/Ari_Holtzman\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2305.14314\n",
      "https://arxiv.org/archive/cs.LG\n",
      "#cite_ref-66\n",
      "https://proceedings.mlr.press/v32/kiros14.html\n",
      "#cite_ref-67\n",
      "https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html\n",
      "#cite_ref-68\n",
      "https://openaccess.thecvf.com/content_iccv_2015/html/Antol_VQA_Visual_Question_ICCV_2015_paper.html\n",
      "#cite_ref-69\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2301.12597\n",
      "https://arxiv.org/archive/cs.CV\n",
      "#cite_ref-70\n",
      "https://proceedings.neurips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2204.14198\n",
      "#cite_ref-71\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2303.03378\n",
      "https://arxiv.org/archive/cs.LG\n",
      "#cite_ref-72\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2304.08485\n",
      "https://arxiv.org/archive/cs.CV\n",
      "#cite_ref-73\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2306.02858\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-74\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2303.08774\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-75\n",
      "https://cdn.openai.com/papers/GPTV_System_Card.pdf\n",
      "#cite_ref-76\n",
      "https://www.youtube.com/watch?v=cNfINi5CNbY&t=931s\n",
      "#cite_ref-fJta3_77-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2203.15556\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-IYm4Q_78-0\n",
      "#cite_ref-IYm4Q_78-1\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2210.14891\n",
      "https://arxiv.org/archive/cs.LG\n",
      "#cite_ref-JM6s1_79-0\n",
      "https://www.jasonwei.net/blog/emergence\n",
      "#cite_ref-Hahn_20230314_80-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2303.07971\n",
      "https://arxiv.org/archive/cs.LG\n",
      "#cite_ref-57FEA_81-0\n",
      "https://aclanthology.org/N19-1128\n",
      "/wiki/Doi_(identifier)\n",
      "https://doi.org/10.18653%2Fv1%2FN19-1128\n",
      "/wiki/S2CID_(identifier)\n",
      "https://api.semanticscholar.org/CorpusID:102353817\n",
      "#cite_ref-TEIkA_82-0\n",
      "https://pilehvar.github.io/wic/\n",
      "#cite_ref-zgy1i_83-0\n",
      "https://openreview.net/forum?id=gJcEM8sxHK\n",
      "#cite_ref-Imb98_84-0\n",
      "https://www.notion.so/A-Closer-Look-at-Large-Language-Models-Emergent-Abilities-493876b55df5479d80686f68a1abd72f\n",
      "#cite_ref-CeQVF_85-0\n",
      "https://www.quantamagazine.org/the-unpredictable-abilities-emerging-from-large-ai-models-20230316/\n",
      "#cite_ref-C775b_86-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2304.15004\n",
      "https://arxiv.org/archive/cs.AI\n",
      "#cite_ref-IZSIr_87-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2210.13382\n",
      "https://arxiv.org/archive/cs.LG\n",
      "#cite_ref-RLik9_88-0\n",
      "https://thegradient.pub/othello/\n",
      "#cite_ref-Hln1l_89-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2305.11169\n",
      "https://arxiv.org/archive/cs.LG\n",
      "#cite_ref-oYGlo_90-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2301.05217\n",
      "https://arxiv.org/archive/cs.LG\n",
      "#cite_ref-debate_understanding_91-0\n",
      "#cite_ref-debate_understanding_91-1\n",
      "#cite_ref-debate_understanding_91-2\n",
      "#cite_ref-debate_understanding_91-3\n",
      "#cite_ref-debate_understanding_91-4\n",
      "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10068812\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2210.13966\n",
      "/wiki/Bibcode_(identifier)\n",
      "https://ui.adsabs.harvard.edu/abs/2023PNAS..12015907M\n",
      "/wiki/Doi_(identifier)\n",
      "https://doi.org/10.1073%2Fpnas.2215907120\n",
      "/wiki/PMC_(identifier)\n",
      "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10068812\n",
      "/wiki/PMID_(identifier)\n",
      "https://pubmed.ncbi.nlm.nih.gov/36943882\n",
      "#cite_ref-O8Upd_92-0\n",
      "https://www.nytimes.com/2023/05/16/technology/microsoft-ai-human-reasoning.html\n",
      "#cite_ref-microsoft_sparks_93-0\n",
      "#cite_ref-microsoft_sparks_93-1\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2303.12712\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-rEEmH_94-0\n",
      "https://www.zdnet.com/article/chatgpt-is-more-like-an-alien-intelligence-than-a-human-brain-says-futurist/\n",
      "#cite_ref-new_yorker_kind_of_mind_95-0\n",
      "#cite_ref-new_yorker_kind_of_mind_95-1\n",
      "https://www.newyorker.com/science/annals-of-artificial-intelligence/what-kind-of-mind-does-chatgpt-have\n",
      "#cite_ref-rAFIZ_96-0\n",
      "https://www.nytimes.com/2023/05/30/technology/shoggoth-meme-ai.html\n",
      "#cite_ref-4luKE_97-0\n",
      "https://time.com/6271657/a-to-z-of-artificial-intelligence/\n",
      "#cite_ref-hallucination-survey_98-0\n",
      "https://dl.acm.org/doi/pdf/10.1145/3571730\n",
      "/wiki/Association_for_Computing_Machinery\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2202.03629\n",
      "/wiki/Doi_(identifier)\n",
      "https://doi.org/10.1145%2F3571730\n",
      "/wiki/S2CID_(identifier)\n",
      "https://api.semanticscholar.org/CorpusID:246652372\n",
      "#cite_ref-99\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2307.03987\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-100\n",
      "/wiki/ISBN_(identifier)\n",
      "/wiki/Special:BookSources/978-0-465-05674-3\n",
      "#cite_ref-101\n",
      "/wiki/ISBN_(identifier)\n",
      "/wiki/Special:BookSources/978-1-107-04396-1\n",
      "#cite_ref-102\n",
      "/wiki/ISBN_(identifier)\n",
      "/wiki/Special:BookSources/978-0-262-36997-8\n",
      "#cite_ref-Huyen_103-0\n",
      "#cite_ref-Huyen_103-1\n",
      "https://thegradient.pub/understanding-evaluation-metrics-for-language-models/\n",
      "#cite_ref-boolq_104-0\n",
      "#cite_ref-boolq_104-1\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/1905.10044\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-survey_105-0\n",
      "#cite_ref-survey_105-1\n",
      "#cite_ref-survey_105-2\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2303.18223\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-bigbench_106-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2206.04615\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-truthfulqa_107-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2109.07958\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-hellaswag_108-0\n",
      "#cite_ref-hellaswag_108-1\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/1905.07830\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-ZDTUM_109-0\n",
      "/wiki/Doi_(identifier)\n",
      "https://doi.org/10.1038%2Fs41551-023-01012-6\n",
      "/wiki/PMID_(identifier)\n",
      "https://pubmed.ncbi.nlm.nih.gov/36882584\n",
      "/wiki/S2CID_(identifier)\n",
      "https://api.semanticscholar.org/CorpusID:257403466\n",
      "#cite_ref-81w7x_110-0\n",
      "https://www.economist.com/finance-and-economics/2023/05/07/your-job-is-probably-safe-from-artificial-intelligence\n",
      "#cite_ref-zIM6Y_111-0\n",
      "https://www.goldmansachs.com/intelligence/pages/generative-ai-could-raise-global-gdp-by-7-percent.html\n",
      "#cite_ref-112\n",
      "https://people.cs.rutgers.edu/~dd903/assets/papers/sigmod23.pdf\n",
      "/wiki/Doi_(identifier)\n",
      "https://doi.org/10.1145%2F3589324\n",
      "/wiki/S2CID_(identifier)\n",
      "https://api.semanticscholar.org/CorpusID:259213212\n",
      "#cite_ref-113\n",
      "#CITEREFPengWangDeng2023\n",
      "#cite_ref-nD6kH_114-0\n",
      "https://www.japantimes.co.jp/news/2023/05/01/business/tech/ai-fake-news-content-farms/\n",
      "#cite_ref-PKiPY_115-0\n",
      "https://www.science.org/content/article/could-chatbots-help-devise-next-pandemic-virus\n",
      "/wiki/Doi_(identifier)\n",
      "https://doi.org/10.1126%2Fscience.adj2463\n",
      "#cite_ref-116\n",
      "https://www.sfgate.com/tech/article/google-openai-chatgpt-break-model-18525445.php\n",
      "#cite_ref-117\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2401.05566\n",
      "https://arxiv.org/archive/cs.CR\n",
      "#cite_ref-:8_118-0\n",
      "#cite_ref-:8_118-1\n",
      "https://www.scientificamerican.com/article/chatgpt-replicates-gender-bias-in-recommendation-letters/\n",
      "#cite_ref-:1_119-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2303.16281v2\n",
      "https://arxiv.org/archive/cs.CY\n",
      "#cite_ref-120\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2305.18189\n",
      "#cite_ref-121\n",
      "https://dl.acm.org/doi/10.1145/3582269.3615599\n",
      "/wiki/Doi_(identifier)\n",
      "https://doi.org/10.1145%2F3582269.3615599\n",
      "/wiki/ISBN_(identifier)\n",
      "/wiki/Special:BookSources/979-8-4007-0113-9\n",
      "#cite_ref-122\n",
      "https://www.technologyreview.com/2023/08/07/1077324/ai-language-models-are-rife-with-political-biases/\n",
      "#cite_ref-oai-unsup_126-0\n",
      "https://openai.com/research/language-unsupervised\n",
      "https://web.archive.org/web/20230318210736/https://openai.com/research/language-unsupervised\n",
      "#cite_ref-gpt1_127-0\n",
      "https://github.com/openai/finetune-transformer-lm\n",
      "#cite_ref-bert-paper_128-0\n",
      "#cite_ref-bert-paper_128-1\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/1810.04805v2\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-bHZJ2_129-0\n",
      "https://www.nextplatform.com/2021/08/24/cerebras-shifts-architecture-to-meet-massive-ai-ml-models/\n",
      "#cite_ref-bert-web_130-0\n",
      "https://github.com/google-research/bert\n",
      "#cite_ref-Ir545_131-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2209.14500\n",
      "https://arxiv.org/archive/cs.LG\n",
      "#cite_ref-:6_132-0\n",
      "#cite_ref-:6_132-1\n",
      "http://jmlr.org/papers/v21/20-074.html\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/1910.10683\n",
      "/wiki/ISSN_(identifier)\n",
      "https://www.worldcat.org/issn/1533-7928\n",
      "#cite_ref-133\n",
      "https://github.com/google-research/text-to-text-transfer-transformer\n",
      "#cite_ref-134\n",
      "https://imagen.research.google/\n",
      "#cite_ref-45rAm_135-0\n",
      "https://www.kdnuggets.com/bert-roberta-distilbert-xlnet-which-one-to-use.html\n",
      "#cite_ref-xlnet_136-0\n",
      "https://github.com/zihangdai/xlnet/\n",
      "#cite_ref-gAbNO_137-0\n",
      "https://analyticsindiamag.com/google-introduces-new-architecture-to-reduce-cost-of-transformers/\n",
      "#cite_ref-LX3rI_138-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/1906.08237\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-15Brelease_139-0\n",
      "https://openai.com/blog/gpt-2-1-5b-release/\n",
      "https://web.archive.org/web/20191114074358/https://openai.com/blog/gpt-2-1-5b-release/\n",
      "#cite_ref-5T8u5_140-0\n",
      "https://openai.com/research/better-language-models\n",
      "#cite_ref-LambdaLabs_141-0\n",
      "#cite_ref-LambdaLabs_141-1\n",
      "https://lambdalabs.com/blog/demystifying-gpt-3\n",
      "#cite_ref-Sudbe_142-0\n",
      "https://github.com/openai/gpt-2\n",
      "#cite_ref-:2_143-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2005.14165v4\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-chatgpt-blog_144-0\n",
      "https://openai.com/blog/chatgpt/\n",
      "#cite_ref-gpt-neo_145-0\n",
      "https://github.com/EleutherAI/gpt-neo\n",
      "#cite_ref-Pile_146-0\n",
      "#cite_ref-Pile_146-1\n",
      "#cite_ref-Pile_146-2\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2101.00027\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-vb-gpt-neo_147-0\n",
      "#cite_ref-vb-gpt-neo_147-1\n",
      "https://venturebeat.com/ai/gpt-3s-free-alternative-gpt-neo-is-something-to-be-excited-about/\n",
      "#cite_ref-JxohJ_148-0\n",
      "https://www.forefront.ai/blog-posts/gpt-j-6b-an-introduction-to-the-largest-open-sourced-gpt-model\n",
      "#cite_ref-:3_149-0\n",
      "#cite_ref-:3_149-1\n",
      "#cite_ref-:3_149-2\n",
      "#cite_ref-:3_149-3\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2304.03208\n",
      "https://arxiv.org/archive/cs.LG\n",
      "#cite_ref-BwnW5_150-0\n",
      "https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/\n",
      "#cite_ref-mtnlg-preprint_151-0\n",
      "#cite_ref-mtnlg-preprint_151-1\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2201.11990\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-qeOB8_152-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2112.12731\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-i8jc4_153-0\n",
      "https://www.anthropic.com/product\n",
      "#cite_ref-AnthroArch_154-0\n",
      "#cite_ref-AnthroArch_154-1\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2112.00861\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-RZqhw_155-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2212.08073\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-mD5eE_156-0\n",
      "https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval\n",
      "#cite_ref-hoffman_157-0\n",
      "#cite_ref-hoffman_157-1\n",
      "#cite_ref-hoffman_157-2\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2203.15556\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-:4_158-0\n",
      "#cite_ref-:4_158-1\n",
      "#cite_ref-:4_158-2\n",
      "#cite_ref-:4_158-3\n",
      "https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf\n",
      "#cite_ref-lamda-blog_159-0\n",
      "#cite_ref-lamda-blog_159-1\n",
      "https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html\n",
      "#cite_ref-DMs9Z_160-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2201.08239\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-gpt-neox-20b_161-0\n",
      "https://aclanthology.org/2022.bigscience-1.9/\n",
      "#cite_ref-chinchilla-blog_162-0\n",
      "#cite_ref-chinchilla-blog_162-1\n",
      "#cite_ref-chinchilla-blog_162-2\n",
      "https://www.deepmind.com/blog/an-empirical-analysis-of-compute-optimal-large-language-model-training\n",
      "#cite_ref-palm-blog_163-0\n",
      "https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html\n",
      "#cite_ref-jlof8_164-0\n",
      "https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/\n",
      "#cite_ref-QjTIc_165-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2205.01068\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-yalm-repo_167-0\n",
      "#cite_ref-yalm-repo_167-1\n",
      "https://github.com/yandex/YaLM-100B\n",
      "#cite_ref-minerva-paper_168-0\n",
      "#cite_ref-minerva-paper_168-1\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2206.14858\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-FfCNK_169-0\n",
      "https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html\n",
      "#cite_ref-bigger-better_170-0\n",
      "https://www.nature.com/articles/d41586-023-00641-w\n",
      "/wiki/Bibcode_(identifier)\n",
      "https://ui.adsabs.harvard.edu/abs/2023Natur.615..202A\n",
      "/wiki/Doi_(identifier)\n",
      "https://doi.org/10.1038%2Fd41586-023-00641-w\n",
      "/wiki/PMID_(identifier)\n",
      "https://pubmed.ncbi.nlm.nih.gov/36890378\n",
      "/wiki/S2CID_(identifier)\n",
      "https://api.semanticscholar.org/CorpusID:257380916\n",
      "#cite_ref-B8wB2_171-0\n",
      "https://huggingface.co/bigscience/bloom\n",
      "#cite_ref-37sY6_172-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2211.09085\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-u5szh_173-0\n",
      "https://www.amazon.science/blog/20b-parameter-alexa-model-sets-new-marks-in-few-shot-learning\n",
      "#cite_ref-HaA7l_174-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2208.01448\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-rpehM_175-0\n",
      "https://aws.amazon.com/blogs/machine-learning/alexatm-20b-is-now-available-in-amazon-sagemaker-jumpstart/\n",
      "#cite_ref-llama-blog_176-0\n",
      "#cite_ref-llama-blog_176-1\n",
      "#cite_ref-llama-blog_176-2\n",
      "https://ai.facebook.com/blog/large-language-model-llama-meta-ai/\n",
      "#cite_ref-:5_177-0\n",
      "#cite_ref-:5_177-1\n",
      "#cite_ref-:5_177-2\n",
      "https://huggingface.co/blog/falcon\n",
      "#cite_ref-KBedq_179-0\n",
      "https://crfm.stanford.edu/2023/03/13/alpaca.html\n",
      "#cite_ref-GPT4Tech_180-0\n",
      "https://cdn.openai.com/papers/gpt-4.pdf\n",
      "/wiki/OpenAI\n",
      "https://web.archive.org/web/20230314190904/https://cdn.openai.com/papers/gpt-4.pdf\n",
      "#cite_ref-D0k2a_182-0\n",
      "https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/\n",
      "#cite_ref-falcon_183-0\n",
      "https://fastcompanyme.com/news/abu-dhabi-based-tii-launches-its-own-version-of-chatgpt/\n",
      "#cite_ref-Xb1gq_184-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2306.01116\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-gzTNw_185-0\n",
      "https://huggingface.co/tiiuae/falcon-40b\n",
      "#cite_ref-Wmlcs_186-0\n",
      "https://www.businesswire.com/news/home/20230531005608/en/UAE's-Falcon-40B-World's-Top-Ranked-AI-Model-from-Technology-Innovation-Institute-is-Now-Royalty-Free\n",
      "#cite_ref-nGOSu_187-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2303.17564\n",
      "https://arxiv.org/archive/cs.LG\n",
      "#cite_ref-9WSFw_188-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2303.10845\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-JiOl8_189-0\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2304.07327\n",
      "https://arxiv.org/archive/cs.CL\n",
      "#cite_ref-190\n",
      "https://www.timesofisrael.com/ai21-labs-rolls-out-new-advanced-ai-language-model-to-rival-openai/\n",
      "#cite_ref-191\n",
      "https://techcrunch.com/2023/04/13/with-bedrock-amazon-enters-the-generative-ai-race/\n",
      "#cite_ref-cnbc-20230516_192-0\n",
      "#cite_ref-cnbc-20230516_192-1\n",
      "https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html\n",
      "/wiki/CNBC\n",
      "#cite_ref-pWyLA_193-0\n",
      "https://blog.google/technology/ai/google-palm-2-ai-large-language-model/\n",
      "#cite_ref-meta-20230719_194-0\n",
      "#cite_ref-meta-20230719_194-1\n",
      "https://ai.meta.com/llama/\n",
      "#cite_ref-195\n",
      "https://www.anthropic.com/index/claude-2\n",
      "#cite_ref-tii-20230921_196-0\n",
      "#cite_ref-tii-20230921_196-1\n",
      "https://falconllm.tii.ae/falcon-180b.html\n",
      "#cite_ref-mistral-20230927_197-0\n",
      "https://mistral.ai/news/announcing-mistral-7b/\n",
      "#cite_ref-198\n",
      "https://www.anthropic.com/index/claude-2-1\n",
      "#cite_ref-199\n",
      "https://github.com/xai-org/grok-1\n",
      "#cite_ref-200\n",
      "https://x.ai/model-card/\n",
      "#cite_ref-201\n",
      "https://deepmind.google/technologies/gemini/#capabilities\n",
      "#cite_ref-202\n",
      "https://venturebeat.com/ai/mistral-shocks-ai-community-as-latest-open-source-model-eclipses-gpt-3-5-performance/\n",
      "#cite_ref-203\n",
      "https://mistral.ai/news/mixtral-of-experts/\n",
      "#cite_ref-204\n",
      "https://mistral.ai/news/mixtral-8x22b/\n",
      "#cite_ref-:9_205-0\n",
      "#cite_ref-:9_205-1\n",
      "https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/\n",
      "#cite_ref-206\n",
      "https://blog.rwkv.com/p/eagle-7b-soaring-past-transformers\n",
      "#cite_ref-207\n",
      "https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#context-window\n",
      "#cite_ref-gemma_208-0\n",
      "https://github.com/google-deepmind/gemma/blob/main/README.md\n",
      "#cite_ref-209\n",
      "https://www.anthropic.com/news/claude-3-family\n",
      "#cite_ref-210\n",
      "https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B\n",
      "#cite_ref-211\n",
      "https://ai.meta.com/blog/meta-llama-3/\n",
      "/w/index.php?title=Large_language_model&action=edit&section=38\n",
      "/wiki/Dan_Jurafsky\n",
      "https://web.stanford.edu/~jurafsky/slp3/ed3book_jan72023.pdf\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2303.18223\n",
      "https://arxiv.org/archive/cs.CL\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2307.10169\n",
      "https://arxiv.org/archive/cs.CL\n",
      "/wiki/ArXiv_(identifier)\n",
      "https://arxiv.org/abs/2306.13549\n",
      "https://arxiv.org/archive/cs.CV\n",
      "https://github.com/eugeneyan/open-llms\n",
      "/wiki/GitHub\n",
      "https://aiindex.stanford.edu/report/\n",
      "https://www.nature.com/articles/s44159-023-00211-x\n",
      "/wiki/Doi_(identifier)\n",
      "https://doi.org/10.1038%2Fs44159-023-00211-x\n",
      "/wiki/ISSN_(identifier)\n",
      "https://www.worldcat.org/issn/2731-0574\n",
      "/wiki/S2CID_(identifier)\n",
      "https://api.semanticscholar.org/CorpusID:259713140\n",
      "/wiki/Template:Natural_language_processing\n",
      "/wiki/Template_talk:Natural_language_processing\n",
      "/wiki/Special:EditPage/Template:Natural_language_processing\n",
      "/wiki/Natural_language_processing\n",
      "/wiki/AI-complete\n",
      "/wiki/Bag-of-words_model\n",
      "/wiki/N-gram\n",
      "/wiki/Bigram\n",
      "/wiki/Trigram\n",
      "/wiki/Computational_linguistics\n",
      "/wiki/Natural-language_understanding\n",
      "/wiki/Stop_word\n",
      "/wiki/Text_processing\n",
      "/wiki/Text_mining\n",
      "/wiki/Argument_mining\n",
      "/wiki/Collocation_extraction\n",
      "/wiki/Concept_mining\n",
      "/wiki/Coreference#Coreference_resolution\n",
      "/wiki/Deep_linguistic_processing\n",
      "/wiki/Distant_reading\n",
      "/wiki/Information_extraction\n",
      "/wiki/Named-entity_recognition\n",
      "/wiki/Ontology_learning\n",
      "/wiki/Parsing\n",
      "/wiki/Semantic_parsing\n",
      "/wiki/Syntactic_parsing_(computational_linguistics)\n",
      "/wiki/Part-of-speech_tagging\n",
      "/wiki/Semantic_analysis_(machine_learning)\n",
      "/wiki/Semantic_role_labeling\n",
      "/wiki/Semantic_decomposition_(natural_language_processing)\n",
      "/wiki/Semantic_similarity\n",
      "/wiki/Sentiment_analysis\n",
      "/wiki/Terminology_extraction\n",
      "/wiki/Text_mining\n",
      "/wiki/Textual_entailment\n",
      "/wiki/Truecasing\n",
      "/wiki/Word-sense_disambiguation\n",
      "/wiki/Word-sense_induction\n",
      "/wiki/Text_segmentation\n",
      "/wiki/Compound-term_processing\n",
      "/wiki/Lemmatisation\n",
      "/wiki/Lexical_analysis\n",
      "/wiki/Shallow_parsing\n",
      "/wiki/Stemming\n",
      "/wiki/Sentence_boundary_disambiguation\n",
      "/wiki/Word#Word_boundaries\n",
      "/wiki/Automatic_summarization\n",
      "/wiki/Multi-document_summarization\n",
      "/wiki/Sentence_extraction\n",
      "/wiki/Text_simplification\n",
      "/wiki/Machine_translation\n",
      "/wiki/Computer-assisted_translation\n",
      "/wiki/Example-based_machine_translation\n",
      "/wiki/Rule-based_machine_translation\n",
      "/wiki/Statistical_machine_translation\n",
      "/wiki/Transfer-based_machine_translation\n",
      "/wiki/Neural_machine_translation\n",
      "/wiki/Distributional_semantics\n",
      "/wiki/BERT_(language_model)\n",
      "/wiki/Document-term_matrix\n",
      "/wiki/Explicit_semantic_analysis\n",
      "/wiki/FastText\n",
      "/wiki/GloVe\n",
      "/wiki/Language_model\n",
      "None\n",
      "/wiki/Latent_semantic_analysis\n",
      "/wiki/Seq2seq\n",
      "/wiki/Word_embedding\n",
      "/wiki/Word2vec\n",
      "/wiki/Language_resource\n",
      "/wiki/Corpus_linguistics\n",
      "/wiki/Lexical_resource\n",
      "/wiki/Linguistic_Linked_Open_Data\n",
      "/wiki/Machine-readable_dictionary\n",
      "/wiki/Parallel_text\n",
      "/wiki/PropBank\n",
      "/wiki/Semantic_network\n",
      "/wiki/Simple_Knowledge_Organization_System\n",
      "/wiki/Speech_corpus\n",
      "/wiki/Text_corpus\n",
      "/wiki/Thesaurus_(information_retrieval)\n",
      "/wiki/Treebank\n",
      "/wiki/Universal_Dependencies\n",
      "/wiki/BabelNet\n",
      "/wiki/Bank_of_English\n",
      "/wiki/DBpedia\n",
      "/wiki/FrameNet\n",
      "/wiki/Google_Ngram_Viewer\n",
      "/wiki/UBY\n",
      "/wiki/WordNet\n",
      "/wiki/Automatic_identification_and_data_capture\n",
      "/wiki/Speech_recognition\n",
      "/wiki/Speech_segmentation\n",
      "/wiki/Speech_synthesis\n",
      "/wiki/Natural_language_generation\n",
      "/wiki/Optical_character_recognition\n",
      "/wiki/Topic_model\n",
      "/wiki/Document_classification\n",
      "/wiki/Latent_Dirichlet_allocation\n",
      "/wiki/Pachinko_allocation\n",
      "/wiki/Computer-assisted_reviewing\n",
      "/wiki/Automated_essay_scoring\n",
      "/wiki/Concordancer\n",
      "/wiki/Grammar_checker\n",
      "/wiki/Predictive_text\n",
      "/wiki/Pronunciation_assessment\n",
      "/wiki/Spell_checker\n",
      "/wiki/Syntax_guessing\n",
      "/wiki/Natural_language_user_interface\n",
      "/wiki/Chatbot\n",
      "/wiki/Interactive_fiction\n",
      "/wiki/Question_answering\n",
      "/wiki/Virtual_assistant\n",
      "/wiki/Voice_user_interface\n",
      "/wiki/Formal_semantics_(natural_language)\n",
      "/wiki/Hallucination_(artificial_intelligence)\n",
      "/wiki/Natural_Language_Toolkit\n",
      "/wiki/SpaCy\n",
      "https://en.wikipedia.org/w/index.php?title=Large_language_model&oldid=1225022647\n",
      "/wiki/Help:Category\n",
      "/wiki/Category:Large_language_models\n",
      "/wiki/Category:Deep_learning\n",
      "/wiki/Category:Natural_language_processing\n",
      "/wiki/Category:CS1:_long_volume_value\n",
      "/wiki/Category:Articles_with_short_description\n",
      "/wiki/Category:Short_description_is_different_from_Wikidata\n",
      "/wiki/Category:Articles_containing_potentially_dated_statements_from_March_2024\n",
      "/wiki/Category:All_articles_containing_potentially_dated_statements\n",
      "/wiki/Category:Articles_containing_potentially_dated_statements_from_2024\n",
      "/wiki/Category:Articles_containing_potentially_dated_statements_from_January_2024\n",
      "/wiki/Category:All_articles_with_unsourced_statements\n",
      "/wiki/Category:Articles_with_unsourced_statements_from_February_2024\n",
      "//en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License\n",
      "//en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License\n",
      "//foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Terms_of_Use\n",
      "//foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy\n",
      "//www.wikimediafoundation.org/\n",
      "https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy\n",
      "/wiki/Wikipedia:About\n",
      "/wiki/Wikipedia:General_disclaimer\n",
      "//en.wikipedia.org/wiki/Wikipedia:Contact_us\n",
      "https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Universal_Code_of_Conduct\n",
      "https://developer.wikimedia.org\n",
      "https://stats.wikimedia.org/#/en.wikipedia.org\n",
      "https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Cookie_statement\n",
      "//en.m.wikipedia.org/w/index.php?title=Large_language_model&mobileaction=toggle_view_mobile\n",
      "https://wikimediafoundation.org/\n",
      "https://www.mediawiki.org/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Large_language_model'\n",
    "response = requests.get(url)\n",
    "\n",
    "# parse the content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "title = soup.title.string\n",
    "\n",
    "# Find all links on the page\n",
    "# extracts the URLs from these tags\n",
    "links = soup.find_all('a')\n",
    "for link in links:\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d19dcd2-5ac5-485d-9aa2-39773d8a30f7",
   "metadata": {},
   "source": [
    "##### pip install faiss-cpu (using FAISS vectorstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0e1b3c-4592-4404-a0f2-01ae3ef677e2",
   "metadata": {},
   "source": [
    "###### FAISS stands for Facebook AI Similarity Search, which is a library for efficient similarity search and clustering of dense vectors. It's commonly used for building large-scale vector stores where vectors can be efficiently indexed and queried based on their similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7363b6d5-5789-4052-b7a4-1c0d1da98bff",
   "metadata": {},
   "source": [
    "##### Using embedding model to ingest documents into a vectorstore (llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9d99a9ba-f256-468b-88a5-13d96366ba65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on my training data, I'd say that some well-known and widely-used Large Language Models (LLMs) include:\\n\\n1. BERT (Bidirectional Encoder Representations from Transformers): Developed by Google, it's a pre-trained language model that's achieved state-of-the-art results in many NLP tasks.\\n2. RoBERTa (Robustly Optimized BERT Pre-training Approach): Another popular LLM developed by Facebook AI, which has shown improved performance over BERT on some benchmarks.\\n3. T5 (Text-to-Text Transformer): A text-to-text transformer model designed for a wide range of NLP tasks, including language translation, question answering, and more.\\n4. DistillBERT: A smaller, more efficient version of BERT that's been distilled from the original model using knowledge distillation.\\n5. Electra: A pre-training task-based approach to LLMs that uses an adversarial training procedure to improve its performance.\\n\\nThese are just a few examples, and there are many other LLM models out there with their own strengths and use cases.\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "\n",
    "\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Large_language_model\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Initialize embeddings and text splitter\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "\n",
    "documents = text_splitter.split_documents(docs)                                              \n",
    "vector = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "# processing pipeline for documents using the create_stuff_documents_chain taking 2 arguments (llm, prompt)\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "\n",
    "\n",
    "document_chain.invoke({\n",
    "    \"input\": \"what are good llm models?\",\n",
    "    \"context\": [Document(page_content=\"Large language model\")]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d7e3ec5c-c228-436c-a95d-684f1783a593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, some good LLM (Large Language Model) models mentioned include:\n",
      "\n",
      "1. Mistral 8x7b: This model has been described as the most powerful open LLM, being more powerful than GPT-3.5 but not as powerful as GPT-4.\n",
      "2. GPT-4 Turbo: This model has a maximum output of 4096 tokens and is capable of generating long conversations.\n",
      "\n",
      "It's worth noting that these models are based on the Transformer architecture, which is currently the most popular architecture for LLMs. Additionally, other models like Google's Gemini 1.5 and Anthropic's Claude 2.1 have also been mentioned as having large context windows and being able to generate long conversations.\n"
     ]
    }
   ],
   "source": [
    "# natural language processing (NLP) pipeline for document retrieval and processing\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "response = retrieval_chain.invoke({\"input\": \"what are good llm models?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f1e08ca4-dbc8-4803-9876-7a9c7eb50063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='^ \"bigscience/bloom  Hugging Face\". huggingface.co.\\n\\n^ Taylor, Ross; Kardas, Marcin; Cucurull, Guillem; Scialom, Thomas; Hartshorn, Anthony; Saravia, Elvis; Poulton, Andrew; Kerkez, Viktor; Stojnic, Robert (16 November 2022). \"Galactica: A Large Language Model for Science\". arXiv:2211.09085 [cs.CL].\\n\\n^ \"20B-parameter Alexa model sets new marks in few-shot learning\". Amazon Science. 2 August 2022.\\n\\n^ Soltan, Saleh; Ananthakrishnan, Shankar; FitzGerald, Jack; et\\xa0al. (3 August 2022). \"AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model\". arXiv:2208.01448 [cs.CL].\\n\\n^ \"AlexaTM 20B is now available in Amazon SageMaker JumpStart | AWS Machine Learning Blog\". aws.amazon.com. 17 November 2022. Retrieved 13 March 2023.\\n\\n^ a b c \"Introducing LLaMA: A foundational, 65-billion-parameter large language model\". Meta AI. 24 February 2023.\\n\\n^ a b c \"The Falcon has landed in the Hugging Face ecosystem\". huggingface.co. Retrieved 2023-06-20.\\n\\n^ \"Stanford CRFM\". crfm.stanford.edu.\\n\\n^ \"GPT-4 Technical Report\" (PDF). OpenAI. 2023. Archived (PDF) from the original on March 14, 2023. Retrieved March 14, 2023.\\n\\n^ Dey, Nolan (March 28, 2023). \"Cerebras-GPT: A Family of Open, Compute-efficient, Large Language Models\". Cerebras.\\n\\n^ \"Abu Dhabi-based TII launches its own version of ChatGPT\". tii.ae.\\n\\n^ Penedo, Guilherme; Malartic, Quentin; Hesslow, Daniel; Cojocaru, Ruxandra; Cappelli, Alessandro; Alobeidli, Hamza; Pannier, Baptiste; Almazrouei, Ebtesam; Launay, Julien (2023-06-01). \"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only\". arXiv:2306.01116 [cs.CL].\\n\\n^ \"tiiuae/falcon-40b  Hugging Face\". huggingface.co. 2023-06-09. Retrieved 2023-06-20.\\n\\n^ UAE\\'s Falcon 40B, World\\'s Top-Ranked AI Model from Technology Innovation Institute, is Now Royalty-Free, 31 May 2023\\n\\n^ Wu, Shijie; Irsoy, Ozan; Lu, Steven; Dabravolski, Vadim; Dredze, Mark; Gehrmann, Sebastian; Kambadur, Prabhanjan; Rosenberg, David; Mann, Gideon (March 30, 2023). \"BloombergGPT: A Large Language Model for Finance\". arXiv:2303.17564 [cs.LG].\\n\\n^ Ren, Xiaozhe; Zhou, Pingyi; Meng, Xinfan; Huang, Xinjing; Wang, Yadao; Wang, Weichao; Li, Pengfei; Zhang, Xiaoda; Podolskiy, Alexander; Arshinov, Grigory; Bout, Andrey; Piontkovskaya, Irina; Wei, Jiansheng; Jiang, Xin; Su, Teng; Liu, Qun; Yao, Jun (March 19, 2023). \"PanGu-: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing\". arXiv:2303.10845 [cs.CL].\\n\\n^ Kpf, Andreas; Kilcher, Yannic; von Rtte, Dimitri; Anagnostidis, Sotiris; Tam, Zhi-Rui; Stevens, Keith; Barhoum, Abdullah; Duc, Nguyen Minh; Stanley, Oliver; Nagyfi, Richrd; ES, Shahul; Suri, Sameer; Glushkov, David; Dantuluri, Arnav; Maguire, Andrew (2023-04-14). \"OpenAssistant Conversations  Democratizing Large Language Model Alignment\". arXiv:2304.07327 [cs.CL].\\n\\n^ Wrobel, Sharon. \"Tel Aviv startup rolls out new advanced AI language model to rival OpenAI\". www.timesofisrael.com. Retrieved 2023-07-24.\\n\\n^ Wiggers, Kyle (2023-04-13). \"With Bedrock, Amazon enters the generative AI race\". TechCrunch. Retrieved 2023-07-24.\\n\\n^ a b Elias, Jennifer (16 May 2023). \"Google\\'s newest A.I. model uses nearly five times more text data for training than its predecessor\". CNBC. Retrieved 18 May 2023.\\n\\n^ \"Introducing PaLM 2\". Google. May 10, 2023.\\n\\n^ a b \"Introducing Llama 2: The Next Generation of Our Open Source Large Language Model\". Meta AI. 2023. Retrieved 2023-07-19.\\n\\n^ \"Claude 2\". anthropic.com. Retrieved 12 December 2023.\\n\\n^ a b \"Falcon 180B\". Technology Innovation Institute. 2023. Retrieved 2023-09-21.\\n\\n^ \"Announcing Mistral 7B\". Mistral. 2023. Retrieved 2023-10-06.\\n\\n^ \"Introducing Claude 2.1\". anthropic.com. Retrieved 12 December 2023.\\n\\n^ xai-org/grok-1, xai-org, 2024-03-19, retrieved 2024-03-19\\n\\n^ \"Grok-1 model card\". x.ai. Retrieved 12 December 2023.\\n\\n^ \"Gemini  Google DeepMind\". deepmind.google. Retrieved 12 December 2023.', metadata={'source': 'https://en.wikipedia.org/wiki/Large_language_model', 'title': 'Large language model - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content='^ Frantar, Elias; Ashkboos, Saleh; Hoefler, Torsten; Alistarh, Dan (2022-10-01). \"GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers\". arXiv:2210.17323 [cs.LG].\\n\\n^ Dettmers, Tim; Svirschevski, Ruslan; Egiazarian, Vage; Kuznedelev, Denis; Frantar, Elias; Ashkboos, Saleh; Borzunov, Alexander; Hoefler, Torsten; Alistarh, Dan (2023-06-01). \"SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression\". arXiv:2306.03078 [cs.CL].\\n\\n^ Dettmers, Tim; Pagnoni, Artidoro; Holtzman, Ari; Zettlemoyer, Luke (2023-05-01). \"QLoRA: Efficient Finetuning of Quantized LLMs\". arXiv:2305.14314 [cs.LG].\\n\\n^ Kiros, Ryan; Salakhutdinov, Ruslan; Zemel, Rich (2014-06-18). \"Multimodal Neural Language Models\". Proceedings of the 31st International Conference on Machine Learning. PMLR: 595603.\\n\\n^ Krizhevsky, Alex; Sutskever, Ilya; Hinton, Geoffrey E (2012). \"ImageNet Classification with Deep Convolutional Neural Networks\". Advances in Neural Information Processing Systems. 25. Curran Associates, Inc.\\n\\n^ Antol, Stanislaw; Agrawal, Aishwarya; Lu, Jiasen; Mitchell, Margaret; Batra, Dhruv; Zitnick, C. Lawrence; Parikh, Devi (2015). \"VQA: Visual Question Answering\". ICCV: 24252433.\\n\\n^ Li, Junnan; Li, Dongxu; Savarese, Silvio; Hoi, Steven (2023-01-01). \"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\". arXiv:2301.12597 [cs.CV].\\n\\n^ Alayrac, Jean-Baptiste; Donahue, Jeff; Luc, Pauline; Miech, Antoine; Barr, Iain; Hasson, Yana; Lenc, Karel; Mensch, Arthur; Millican, Katherine; Reynolds, Malcolm; Ring, Roman; Rutherford, Eliza; Cabi, Serkan; Han, Tengda; Gong, Zhitao (2022-12-06). \"Flamingo: a Visual Language Model for Few-Shot Learning\". Advances in Neural Information Processing Systems. 35: 2371623736. arXiv:2204.14198.\\n\\n^ Driess, Danny; Xia, Fei; Sajjadi, Mehdi S. M.; Lynch, Corey; Chowdhery, Aakanksha; Ichter, Brian; Wahid, Ayzaan; Tompson, Jonathan; Vuong, Quan; Yu, Tianhe; Huang, Wenlong; Chebotar, Yevgen; Sermanet, Pierre; Duckworth, Daniel; Levine, Sergey (2023-03-01). \"PaLM-E: An Embodied Multimodal Language Model\". arXiv:2303.03378 [cs.LG].\\n\\n^ Liu, Haotian; Li, Chunyuan; Wu, Qingyang; Lee, Yong Jae (2023-04-01). \"Visual Instruction Tuning\". arXiv:2304.08485 [cs.CV].\\n\\n^ Zhang, Hang; Li, Xin; Bing, Lidong (2023-06-01). \"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding\". arXiv:2306.02858 [cs.CL].\\n\\n^ OpenAI (2023-03-27). \"GPT-4 Technical Report\". arXiv:2303.08774 [cs.CL].\\n\\n^ OpenAI (September 25, 2023). \"GPT-4V(ision) System Card\" (PDF).\\n\\n^ Pichai, Sundar, Google Keynote (Google I/O \\'23), timestamp 15:31, retrieved 2023-07-02\\n\\n^ Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; Buchatskaya, Elena; Cai, Trevor; Rutherford, Eliza; Casas, Diego de Las; Hendricks, Lisa Anne; Welbl, Johannes; Clark, Aidan; Hennigan, Tom; Noland, Eric; Millican, Katie; Driessche, George van den; Damoc, Bogdan (2022-03-29). \"Training Compute-Optimal Large Language Models\". arXiv:2203.15556 [cs.CL].\\n\\n^ a b Caballero, Ethan; Gupta, Kshitij; Rish, Irina; Krueger, David (2022). \"Broken Neural Scaling Laws\". arXiv:2210.14891 [cs.LG].\\n\\n^ \"137 emergent abilities of large language models\". Jason Wei. Retrieved 2023-06-24.\\n\\n^ Hahn, Michael; Goyal, Navin (2023-03-14). \"A Theory of Emergent In-Context Learning as Implicit Structure Induction\". arXiv:2303.07971 [cs.LG].\\n\\n^ Pilehvar, Mohammad Taher; Camacho-Collados, Jose (June 2019). \"Proceedings of the 2019 Conference of the North\". Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Minneapolis, Minnesota: Association for Computational Linguistics: 12671273. doi:10.18653/v1/N19-1128. S2CID\\xa0102353817.\\n\\n^ \"WiC: The Word-in-Context Dataset\". pilehvar.github.io. Retrieved 2023-06-27.', metadata={'source': 'https://en.wikipedia.org/wiki/Large_language_model', 'title': 'Large language model - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content=\"Training cost[edit]\\nAdvances in software and hardware have reduced the cost substantially since 2020, such that in 2023 training of a 12-billion-parameter LLM computational cost is 72,300 A100-GPU-hours, while in 2020 the cost of training a 1.5-billion-parameter LLM (which was two orders of magnitude smaller than the state of the art in 2020) was between $80 thousand and $1.6 million.[40][41][42] Since 2020, large sums were invested in increasingly large models. For example, training of the GPT-2 (i.e. a 1.5-billion-parameters model) in 2019 cost $50,000, while training of the PaLM (i.e. a 540-billion-parameters model) in 2022 cost $8 million, and Megatron-Turing NLG 530B (in 2021) cost around $11 million.[43]\\nFor Transformer-based LLM, training cost is much higher than inference cost. It costs 6 FLOPs per parameter to train on one token, whereas it costs 1 to 2 FLOPs per parameter to infer on one token.[44]\\n\\nTool use[edit]\\nThere are certain tasks that, in principle, cannot be solved by any LLM, at least not without the use of external tools or additional software. An example of such a task is responding to the user's input '354 * 139 = ', provided that the LLM has not already encountered a continuation of this calculation in its training corpus. In such cases, the LLM needs to resort to running program code that calculates the result, which can then be included in its response. Another example is 'What is the time now? It is ', where a separate program interpreter would need to execute a code to get system time on the computer, so LLM could include it in its reply.[45][46] This basic strategy can be sophisticated with multiple attempts of generated programs, and other sampling strategies.[47]\\nGenerally, in order to get an LLM to use tools, one must finetune it for tool-use. If the number of tools is finite, then finetuning may be done just once. If the number of tools can grow arbitrarily, as with online API services, then the LLM can be fine-tuned to be able to read API documentation and call API correctly.[48][49]\\nA simpler form of tool use is Retrieval Augmented Generation: augment an LLM with document retrieval, sometimes using a vector database. Given a query, a document retriever is called to retrieve the most relevant (usually measured by first encoding the query and the documents into vectors, then finding the documents with vectors closest in Euclidean norm to the query vector). The LLM then generates an output based on both the query and the retrieved documents.[50]\", metadata={'source': 'https://en.wikipedia.org/wiki/Large_language_model', 'title': 'Large language model - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content='Evaluation[edit]\\nPerplexity[edit]\\nThe most commonly used measure of a language model\\'s performance is its perplexity on a given text corpus. Perplexity is a measure of how well a model is able to predict the contents of a dataset; the higher the likelihood the model assigns to the dataset, the lower the perplexity. Mathematically, perplexity is defined as the exponential of the average negative log likelihood per token:\\n\\n\\n\\nlog\\n\\u2061\\n(\\n\\nPerplexity\\n\\n)\\n=\\n\\n\\n\\n1\\nN\\n\\n\\n\\n\\n\\ni\\n=\\n1\\n\\n\\nN\\n\\n\\nlog\\n\\u2061\\n(\\nPr\\n(\\n\\n\\ntoken\\n\\n\\ni\\n\\n\\n\\n\\n\\ncontext for token\\n\\n\\ni\\n\\n\\n)\\n)\\n\\n\\n{\\\\displaystyle \\\\log({\\\\text{Perplexity}})=-{\\\\frac {1}{N}}\\\\sum _{i=1}^{N}\\\\log(\\\\Pr({\\\\text{token}}_{i}\\\\mid {\\\\text{context for token}}_{i}))}\\n\\nhere \\n\\n\\n\\nN\\n\\n\\n{\\\\displaystyle N}\\n\\n is the number of tokens in the text corpus, and \"context for token \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n\" depends on the specific type of LLM used. If the LLM is autoregressive, then \"context for token \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n\" is the segment of text appearing before token \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n. If the LLM is masked, then \"context for token \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n\" is the segment of text surrounding token \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n.\\nBecause language models may overfit to their training data, models are usually evaluated by their perplexity on a test set of unseen data.[39] This presents particular challenges for the evaluation of large language models. As they are trained on increasingly large corpora of text largely scraped from the web, it becomes increasingly likely that models\\' training data inadvertently includes portions of any given test set.[3]\\n\\nBPW, BPC, and BPT[edit]\\nIn information theory, the concept of entropy is intricately linked to perplexity, a relationship notably established by Claude Shannon.[103] This relationship is mathematically expressed as \\n\\n\\n\\n\\nEntropy\\n\\n=\\n\\nlog\\n\\n2\\n\\n\\n\\u2061\\n(\\n\\nPerplexity\\n\\n)\\n\\n\\n{\\\\displaystyle {\\\\text{Entropy}}=\\\\log _{2}({\\\\text{Perplexity}})}\\n\\n.\\nEntropy, in this context, is commonly quantified in terms of bits per word (BPW) or bits per character (BPC), which hinges on whether the language model utilizes word-based or character-based tokenization.\\nNotably, in the case of larger language models that predominantly employ sub-word tokenization, bits per token (BPT) emerges as a seemingly more appropriate measure. However, due to the variance in tokenization methods across different Large Language Models (LLMs), BPT does not serve as a reliable metric for comparative analysis among diverse models. To convert BPT into BPW, one can multiply it by the average number of tokens per word.\\nIn the evaluation and comparison of language models, cross-entropy is generally the preferred metric over entropy. The underlying principle is that a lower BPW is indicative of a model\\'s enhanced capability for compression. This, in turn, reflects the model\\'s proficiency in making accurate predictions.', metadata={'source': 'https://en.wikipedia.org/wiki/Large_language_model', 'title': 'Large language model - Wikipedia', 'language': 'en'})]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "# # Define the prompt template with placeholders for chat history and input text\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "      MessagesPlaceholder(variable_name=\"chat_history\"), \n",
    "        (\"user\", \"{input}\"),\n",
    "        (\"user\", \"Given the above conversation, generate a search query to look up to get information relevant to the conversation\")\n",
    "    ])\n",
    "\n",
    "# Create the history-aware retriever by passing both the retriever and the prompt template\n",
    "history_aware_retriever = create_history_aware_retriever(llm, retriever, prompt)\n",
    "\n",
    "\n",
    "chat_history = [HumanMessage(content=\"what embeddings perform?\"), AIMessage(content=\"Yes!\")]\n",
    "retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"what are embeddings\"\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f7d822-8458-4e56-a792-5dee37afeed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
